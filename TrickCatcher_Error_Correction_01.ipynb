{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1e97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1efcdab9",
   "metadata": {},
   "source": [
    "# EvalPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed23ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì´ 164ê°œì˜ HumanEval ì›ë³¸ ë¬¸ì œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ.\n",
      "---\n",
      "### ğŸ§© Task ID: HumanEval/0 ë°ì´í„° êµ¬ì¡°\n",
      "1. Prompt (ë¬¸ì œ ë° ì‹œê·¸ë‹ˆì²˜):\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:...\n",
      "\n",
      "2. Canonical Solution (ì •ë‹µ ì½”ë“œ):\n",
      "sorted_numbers = sorted(numbers)\n",
      "    for i in range(len(sorted_numbers) - 1):\n",
      "        if sorte...\n",
      "\n",
      "3. Entry Point (í•¨ìˆ˜ ì´ë¦„): has_close_elements\n",
      "\n",
      "4. Test Code (ì›ë³¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸):\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candid...\n",
      "\n",
      "---\n",
      "â¡ï¸ ëª¨ë“  HumanEval ë¬¸ì œê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
      "âœ… ì´ 164ê°œì˜ HumanEval+ ë¬¸ì œ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ì¶œ ì‹œì‘.\n",
      "---\n",
      "### ğŸ§ª Task ID: HumanEval/0\n",
      "Test Code Snippet:\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]...\n",
      "\n",
      "### ğŸ§ª Task ID: HumanEval/1\n",
      "Test Code Snippet:\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate('(()()) ((())) () ((())()())') == [\n",
      "        '(()())', '((()))', '()', '((())()())'\n",
      "    ]\n",
      "    ass...\n",
      "\n",
      "### ğŸ§ª Task ID: HumanEval/2\n",
      "Test Code Snippet:\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate(3.5) == 0.5\n",
      "    assert abs(candidate(1.33) - 0.33) < 1e-6\n",
      "    assert abs(candidate(123.456) - 0....\n",
      "\n",
      "---\n",
      "â¡ï¸ ëª¨ë“  164ê°œ ë¬¸ì œì˜ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ì¶œ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# pip install evalplus\n",
    "\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from evalplus.data import get_human_eval_plus\n",
    "\n",
    "def extract_all_human_eval_data() -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    EvalPlus íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ HumanEval ì›ë³¸ ë°ì´í„°ì…‹ì˜ ëª¨ë“  ë¬¸ì œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # HumanEval ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        # ê²°ê³¼ëŠ” {'HumanEval/0': {...}, 'HumanEval/1': {...}, ...} í˜•íƒœì…ë‹ˆë‹¤.\n",
    "        problems: Dict[str, Dict[str, Any]] = get_human_eval_plus()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: HumanEval ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. 'evalplus' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤. ({e})\")\n",
    "        return {}\n",
    "\n",
    "    num_problems = len(problems)\n",
    "    print(f\"âœ… ì´ {num_problems}ê°œì˜ HumanEval ì›ë³¸ ë¬¸ì œ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ.\")\n",
    "    print(\"---\")\n",
    "\n",
    "    # ì˜ˆì‹œë¡œ ì²« ë²ˆì§¸ ë¬¸ì œì˜ ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ì—¬ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "    example_task_id = \"HumanEval/0\"\n",
    "    if example_task_id in problems:\n",
    "        problem_data = problems[example_task_id]\n",
    "        \n",
    "        print(f\"### ğŸ§© Task ID: {example_task_id} ë°ì´í„° êµ¬ì¡°\")\n",
    "        print(f\"1. Prompt (ë¬¸ì œ ë° ì‹œê·¸ë‹ˆì²˜):\\n{problem_data['prompt'][:100].strip()}...\\n\")\n",
    "        print(f\"2. Canonical Solution (ì •ë‹µ ì½”ë“œ):\\n{problem_data['canonical_solution'][:100].strip()}...\\n\")\n",
    "        print(f\"3. Entry Point (í•¨ìˆ˜ ì´ë¦„): {problem_data['entry_point']}\\n\")\n",
    "        print(f\"4. Test Code (ì›ë³¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸):\\n{problem_data['test'][:100].strip()}...\\n\")\n",
    "        \n",
    "    print(\"---\")\n",
    "    print(\"â¡ï¸ ëª¨ë“  HumanEval ë¬¸ì œê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    return problems\n",
    "\n",
    "def extract_all_evalplus_test_codes() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    EvalPlus HumanEval+ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë“  ë¬¸ì œì˜ Task IDì™€ Full Test Codeë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # HumanEval+ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "        # ê²°ê³¼ëŠ” {'HumanEval/0': {...}, 'HumanEval/1': {...}, ...} í˜•íƒœì…ë‹ˆë‹¤.\n",
    "        problems: Dict[str, Dict[str, Any]] = get_human_eval_plus()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: EvalPlus ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. 'evalplus' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤. ({e})\")\n",
    "        return {}\n",
    "\n",
    "    all_test_codes: Dict[str, str] = {}\n",
    "\n",
    "    print(f\"âœ… ì´ {len(problems)}ê°œì˜ HumanEval+ ë¬¸ì œ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ì¶œ ì‹œì‘.\")\n",
    "    print(\"---\")\n",
    "\n",
    "    for task_id, problem_data in problems.items():\n",
    "        # 1. 'test' í•„ë“œì— í¬í•¨ëœ ì „ì²´ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "        # ì´ ë¬¸ìì—´ì´ HumanEvalì˜ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ì™€ EvalPlusì˜ ì¶”ê°€(LLM/ë®¤í…Œì´ì…˜ ìƒì„±) í…ŒìŠ¤íŠ¸ë¥¼ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "        test_code_full = problem_data.get('test', 'N/A')\n",
    "        \n",
    "        # 2. ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "        all_test_codes[task_id] = test_code_full\n",
    "        \n",
    "        # 3. ì˜ˆì‹œ ì¶œë ¥ (ì²« 3ê°œë§Œ ê°„ê²°í•˜ê²Œ ì¶œë ¥)\n",
    "        if len(all_test_codes) <= 3:\n",
    "            print(f\"### ğŸ§ª Task ID: {task_id}\")\n",
    "            # í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ ê¸¸ë¯€ë¡œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "            print(f\"Test Code Snippet:\\n{test_code_full[:200].strip()}...\\n\")\n",
    "            \n",
    "    print(\"---\")\n",
    "    print(f\"â¡ï¸ ëª¨ë“  {len(all_test_codes)}ê°œ ë¬¸ì œì˜ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ì¶œ ì™„ë£Œ.\")\n",
    "    \n",
    "    return all_test_codes\n",
    "\n",
    "# HumanEval/0 ë¬¸ì œì— ëŒ€í•œ ë°ì´í„° ì¶”ì¶œ ì‹¤í–‰\n",
    "extracted_human_codes = extract_all_human_eval_data()\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "extracted_test_codes = extract_all_evalplus_test_codes()\n",
    "\n",
    "# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ (ì„ íƒ ì‚¬í•­)\n",
    "# with open('evalplus_test_codes.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(extracted_test_codes, f, indent=4)\n",
    "# print(\"\\në°ì´í„°ê°€ 'evalplus_test_codes.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e2216",
   "metadata": {},
   "source": [
    "# QuixBugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e4b78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracting code from: python_testcases\n",
      "   -> Successfully extracted 42 files.\n",
      "âœ… Extracting code from: python_programs\n",
      "   -> Successfully extracted 50 files.\n",
      "\n",
      "--- Extracted Data Summary ---\n",
      "\n",
      "### ğŸ§ª Python Testcases (Example: load_testdata.py)\n",
      "import json\n",
      "from pathlib import Path\n",
      "\n",
      "\n",
      "def load_json_testcases(algorithm):\n",
      "\n",
      "    quixbugs_root = Path(__file__).parent / \"..\"\n",
      "    testdata_path = quixbugs_root / f\"json_testcases/{algorithm}.json\"\n",
      "    with open(testdata_path) as data_file:\n",
      "        testdata = [json.loads(line) for line in data_file]\n",
      "[... í›„ëµ ...]\n",
      "\n",
      "### ğŸ§ª Python Programs (Example: bitcount.py)\n",
      "def bitcount(n):\n",
      "    count = 0\n",
      "    while n:\n",
      "        n ^= n - 1\n",
      "        count += 1\n",
      "    return count\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Bitcount\n",
      "bitcount\n",
      "\n",
      "\n",
      "Input:\n",
      "    n: a nonnegative int\n",
      "\n",
      "Output:\n",
      "    The number of 1-bits in the binary encoding of n\n",
      "\n",
      "Examples:\n",
      "    >>> bitcount(127)\n",
      "    7\n",
      "    >>> bitcount(128)\n",
      "    1\n",
      "\"\"\"\n",
      "[... í›„ëµ ...]\n"
     ]
    }
   ],
   "source": [
    "#git clone https://github.com/jkoppel/QuixBugs.git\n",
    "\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "# --- ì„¤ì • (ë¡œì»¬ í´ë¡  ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”) ---\n",
    "# QuixBugs ì €ì¥ì†Œë¥¼ í´ë¡ í•œ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "# QUIXBUGS_ROOT_DIR = \"/local_datasets/AACommu/QuixBugs\" # git cloneí•œ QuixBug ì €ì¥ì†Œì˜ Path\n",
    "QUIXBUGS_ROOT_DIR = \"./QuixBugs\"\n",
    "\n",
    "# ì¶”ì¶œ ëŒ€ìƒ ì„œë¸Œ ë””ë ‰í† ë¦¬\n",
    "TARGET_DIRS = [\"python_testcases\", \"python_programs\"]\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def extract_python_code_from_dirs(root_dir: str, target_dirs: List[str]) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ë‚´ì˜ ëŒ€ìƒ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  .py íŒŒì¼ì˜ ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    (ì €ì¥ ë¡œì§ì€ ë¬¸ìì—´ë§Œ ì €ì¥í•˜ë„ë¡ ìœ ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.)\n",
    "    \"\"\"\n",
    "    extracted_data: Dict[str, Dict[str, str]] = {}\n",
    "\n",
    "    if not os.path.isdir(root_dir):\n",
    "        print(f\"âŒ Error: Root directory not found at '{root_dir}'. Please clone the QuixBugs repo first.\")\n",
    "        return extracted_data\n",
    "\n",
    "    for target_dir_name in target_dirs:\n",
    "        target_path = os.path.join(root_dir, target_dir_name)\n",
    "        \n",
    "        if not os.path.isdir(target_path):\n",
    "            print(f\"âš ï¸ Warning: Target directory '{target_path}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        file_contents: Dict[str, str] = {}\n",
    "        \n",
    "        print(f\"âœ… Extracting code from: {target_dir_name}\")\n",
    "        \n",
    "        for dirpath, _, filenames in os.walk(target_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".py\"):\n",
    "                    full_path = os.path.join(dirpath, filename)\n",
    "                    \n",
    "                    try:\n",
    "                        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                            code_content = f.read()\n",
    "                            # íŒŒì¼ ë‚´ìš©ì€ ë°˜ë“œì‹œ ë¬¸ìì—´ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "                            file_contents[filename] = code_content\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ Error reading file {full_path}: {e}\")\n",
    "\n",
    "        extracted_data[target_dir_name] = file_contents\n",
    "        print(f\"   -> Successfully extracted {len(file_contents)} files.\")\n",
    "        \n",
    "    return extracted_data\n",
    "\n",
    "def safe_extract_and_print_code(data_dict: Dict[str, str], dict_name: str):\n",
    "    \"\"\"\n",
    "    ë”•ì…”ë„ˆë¦¬ì—ì„œ ì²« ë²ˆì§¸ í•­ëª©ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    íŠœí”Œ ê°ì²´ê°€ ë“¤ì–´ì™€ë„ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‹œë„í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not data_dict:\n",
    "        print(f\"### ğŸ§ª {dict_name} ë°ì´í„° ì—†ìŒ\")\n",
    "        return\n",
    "\n",
    "    # ì²« ë²ˆì§¸ íŒŒì¼ ì´ë¦„ ì¶”ì¶œ\n",
    "    first_file = next(iter(data_dict.keys()), None)\n",
    "    if not first_file:\n",
    "        return\n",
    "\n",
    "    raw_content = data_dict[first_file]\n",
    "    code_content = None\n",
    "\n",
    "    try:\n",
    "        # 1. ë¬¸ìì—´ì¸ ê²½ìš°: ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        if isinstance(raw_content, str):\n",
    "            code_content = raw_content\n",
    "        # 2. íŠœí”Œì¸ ê²½ìš°: ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ì‚¬ìš© (ì˜¤ë¥˜ì˜ ì›ì¸ì´ì—ˆìŒ)\n",
    "        elif isinstance(raw_content, tuple) and raw_content and isinstance(raw_content[0], str):\n",
    "            code_content = raw_content[0]\n",
    "        # 3. ê·¸ ì™¸ì˜ ê²½ìš°: ì¶œë ¥í•  ìˆ˜ ì—†ìŒì„ ì•Œë¦¼\n",
    "        else:\n",
    "            print(f\"âš ï¸ Warning: {first_file}ì˜ ë‚´ìš©ì´ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì…ë‹ˆë‹¤: {type(raw_content)}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\n### ğŸ§ª {dict_name} (Example: {first_file})\")\n",
    "        # 300ìê¹Œì§€ ìë¥´ê³  .strip()ì„ ì ìš©í•˜ì—¬ ì¶œë ¥\n",
    "        print(code_content[:300].strip() + \"\\n[... í›„ëµ ...]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing content for {first_file}: {e}\")\n",
    "\n",
    "\n",
    "# --- ì½”ë“œ ì¶”ì¶œ ì‹¤í–‰ ---\n",
    "extracted_code = extract_python_code_from_dirs(QUIXBUGS_ROOT_DIR, TARGET_DIRS)\n",
    "\n",
    "## ì¶”ì¶œëœ ë°ì´í„° í™•ì¸\n",
    "\n",
    "print(\"\\n--- Extracted Data Summary ---\")\n",
    "\n",
    "if extracted_code:\n",
    "    # python_testcases ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶œë ¥\n",
    "    testcases = extracted_code.get(\"python_testcases\", {})\n",
    "    safe_extract_and_print_code(testcases, \"Python Testcases\")\n",
    "\n",
    "    # python_programs ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶œë ¥\n",
    "    programs = extracted_code.get(\"python_programs\", {})\n",
    "    safe_extract_and_print_code(programs, \"Python Programs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a16684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_human_codes\n",
    "#extracted_test_codes\n",
    "#extracted_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4fe44",
   "metadata": {},
   "source": [
    "# TrickCatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2014b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.44it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully! ğŸš€\n",
      "âœ… Extracting code from: python_testcases\n",
      "   -> Successfully extracted 42 files.\n",
      "âœ… Extracting code from: python_programs\n",
      "   -> Successfully extracted 50 files.\n",
      "Loading QuixBugs data...\n",
      "âœ… QuixBugs ë¡œë”© ì™„ë£Œ: 41ê°œ ë¬¸ì œ\n",
      "\n",
      "--- Processing QuixBugs PID: bitcount ---\n",
      "ğŸ¤– Generating repaired program variant #1 for bitcount...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Found 1 code blocks. Extracting the last one.\n",
      "   âœ… Saved repaired code to: Outputs\\GenProgs\\bitcount\\variant_1.py\n",
      "ğŸ¤– Generating repaired program variant #2 for bitcount...\n",
      "[Debug] Found 1 code blocks. Extracting the last one.\n",
      "   âœ… Saved repaired code to: Outputs\\GenProgs\\bitcount\\variant_2.py\n",
      "ğŸ¤– Generating repaired program variant #3 for bitcount...\n",
      "[Debug] Found 1 code blocks. Extracting the last one.\n",
      "   âœ… Saved repaired code to: Outputs\\GenProgs\\bitcount\\variant_3.py\n",
      "\n",
      "--- Processing QuixBugs PID: breadth_first_search ---\n",
      "ğŸ¤– Generating repaired program variant #1 for breadth_first_search...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1668\u001b[0m\n\u001b[0;32m   1666\u001b[0m output_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1667\u001b[0m num_variants \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m-> 1668\u001b[0m \u001b[43mcreate_quixbugs_variants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquixbugs_problems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_base_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_variants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;66;03m# 3. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m num_test_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 455\u001b[0m, in \u001b[0;36mcreate_quixbugs_variants\u001b[1;34m(problems, output_base_path, k)\u001b[0m\n\u001b[0;32m    447\u001b[0m prog_messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    448\u001b[0m     {\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_text,\n\u001b[0;32m    451\u001b[0m     }\n\u001b[0;32m    452\u001b[0m ]\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆëŠ” í•¨ìˆ˜ë¼ê³  ê°€ì •\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m variant_code \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprog_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m variant_file \u001b[38;5;241m=\u001b[39m variant_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariant_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(variant_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[6], line 100\u001b[0m, in \u001b[0;36mgenerate_code_response\u001b[1;34m(messages, max_new_tokens)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     93\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m TOKENIZER\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[0;32m     94\u001b[0m         messages,\n\u001b[0;32m     95\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(MODEL\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m MODEL\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    102\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    103\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# âœ… ê²°ì •ë¡ ì  ìƒì„± - temperature ë¬´ì‹œë¨\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mTOKENIZER\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m    105\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mTOKENIZER\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    108\u001b[0m     response_ids \u001b[38;5;241m=\u001b[39m outputs[:, inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[0;32m    109\u001b[0m     full_response_text \u001b[38;5;241m=\u001b[39m TOKENIZER\u001b[38;5;241m.\u001b[39mbatch_decode(response_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\generation\\utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2220\u001b[0m     )\n\u001b[0;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2224\u001b[0m         input_ids,\n\u001b[0;32m   2225\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2226\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2227\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2228\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2229\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2230\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2231\u001b[0m     )\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2243\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\generation\\utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3218\u001b[0m     outputs,\n\u001b[0;32m   3219\u001b[0m     model_kwargs,\n\u001b[0;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3221\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    857\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    858\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    859\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    860\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    861\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    862\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    863\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    864\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    865\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    866\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    868\u001b[0m )\n\u001b[0;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    569\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m         position_embeddings,\n\u001b[0;32m    577\u001b[0m     )\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    580\u001b[0m         hidden_states,\n\u001b[0;32m    581\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    582\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    583\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    584\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    585\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    586\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    587\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    589\u001b[0m     )\n\u001b[0;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:276\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    279\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:57\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 57\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\accelerate\\hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\training\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# ----------------------------\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from typing import List, Dict, Union, Any\n",
    "\n",
    "# ----------------------------\n",
    "# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# ----------------------------\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "# ----------------------------------------------------\n",
    "print(\"Loading model and tokenizer...\")\n",
    "# MODEL_IDë¥¼ Qwen Coder AWQ ëª¨ë¸ë¡œ ë³€ê²½\n",
    "#MODEL_ID = \"Qwen/Qwen2.5-Coder-32B-Instruct-AWQ\" \n",
    "# MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID,\n",
    "#     torch_dtype=\"auto\",        # AWQ ëª¨ë¸ë„ autoë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "#     trust_remote_code=True,  # Qwen ëª¨ë¸ ì‹¤í–‰ ì‹œ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "#     device_map=\"auto\"        # GPU VRAMì— ë§ê²Œ ëª¨ë¸ì„ ìë™ ë°°ì¹˜í•©ë‹ˆë‹¤.\n",
    "# )\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,  # ë˜ëŠ” \"auto\"\n",
    "    device_map=\"auto\",           # 12GB GPU í•œ ì¥ì´ë©´ ì•Œì•„ì„œ ë‹¤ ì˜¬ë ¤ì¤Œ\n",
    ")\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "print(\"Model and tokenizer loaded successfully! ğŸš€\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. LLM í˜¸ì¶œì„ ìœ„í•œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ ì •ì˜\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def extract_code(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ ì „ì²´ ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ C++ ì½”ë“œ ë¸”ë¡ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if \"<think>\" in text:\n",
    "        print(\"[Debug] '<think>' tag found. Skipping markdown code block search.\")\n",
    "        start_match = re.search(r\"#include\", text)\n",
    "        if not start_match:\n",
    "            print(\"[Warning] No '#include' found.\")\n",
    "            return text\n",
    "\n",
    "        start_index = start_match.start()\n",
    "        think_match = re.search(r\"<think>\", text)\n",
    "        search_area_end_index = think_match.start()\n",
    "\n",
    "        search_area = text[start_index:search_area_end_index]\n",
    "        last_brace_index_in_area = search_area.rfind(\"}\")\n",
    "\n",
    "        if last_brace_index_in_area != -1:\n",
    "            print(\"[Debug] Extracted code from '#include' to last '}' before '<think>'.\")\n",
    "            return search_area[:last_brace_index_in_area + 1].strip()\n",
    "        \n",
    "        print(\"[Warning] Could not find closing '}' before <think>.\")\n",
    "        return search_area.strip()\n",
    "\n",
    "    pattern = r\"```(?:cpp)?\\s*(.*?)\\s*```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        print(f\"[Debug] Found {len(matches)} code blocks. Extracting the last one.\")\n",
    "        return matches[-1].strip()\n",
    "\n",
    "    print(\"[Warning] No code block found at all.\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_code_response(\n",
    "    messages: List[Dict[str, str]],\n",
    "    max_new_tokens: int = 3500\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    ì½”ë“œ ìƒì„± ì „ìš©: ì£¼ì–´ì§„ ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì˜ ì‘ë‹µì„ ìƒì„±í•˜ê³  'ì½”ë“œ'ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    do_sample=Falseë¡œ ì„¤ì •í•˜ì—¬ ê²°ì •ë¡ ì  ìƒì„± (temperature ë¬´ì‹œë¨)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = TOKENIZER.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(MODEL.device)\n",
    "\n",
    "        outputs = MODEL.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # âœ… ê²°ì •ë¡ ì  ìƒì„± - temperature ë¬´ì‹œë¨\n",
    "            eos_token_id=TOKENIZER.eos_token_id,\n",
    "            pad_token_id=TOKENIZER.eos_token_id\n",
    "        )\n",
    "\n",
    "        response_ids = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "        full_response_text = TOKENIZER.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    extracted_code = extract_code(full_response_text)\n",
    "    \n",
    "    return extracted_code\n",
    "\n",
    "\n",
    "def generate_text_response(\n",
    "    messages: List[Dict[str, str]],\n",
    "    max_new_tokens: int = 512\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    ì¼ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ì „ìš©: í…ŒìŠ¤íŠ¸ ì…ë ¥ ë“±ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©.\n",
    "    ì½”ë“œ ì¶”ì¶œ ì—†ì´ ì›ë³¸ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    do_sample=Falseë¡œ ì„¤ì •í•˜ì—¬ ê²°ì •ë¡ ì  ìƒì„±\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = TOKENIZER.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(MODEL.device)\n",
    "\n",
    "        outputs = MODEL.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # âœ… ê²°ì •ë¡ ì  ìƒì„±\n",
    "            eos_token_id=TOKENIZER.eos_token_id,\n",
    "            pad_token_id=TOKENIZER.eos_token_id\n",
    "        )\n",
    "\n",
    "        response_ids = outputs[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "        full_response_text = TOKENIZER.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return full_response_text\n",
    "\n",
    "\n",
    "def _read_text_if_exists(path: Union[Path, None]) -> str:\n",
    "    \"\"\"íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ í…ìŠ¤íŠ¸ë¥¼ ì½ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if path and path.exists():\n",
    "        try:\n",
    "            return path.read_text(encoding=\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Could not read file {path}: {e}\")\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def load_trickybugs_data_revised(base_path: str, lang: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    TrickyBugs íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"Loading TrickyBugs data...\")\n",
    "\n",
    "    base = Path(base_path)\n",
    "    problems_dir = base / \"problems\"\n",
    "\n",
    "    if not problems_dir.exists():\n",
    "        raise FileNotFoundError(f\"Problems directory not found at: {problems_dir}\")\n",
    "\n",
    "    loaded_problems = {}\n",
    "    pid_dirs = sorted([p for p in problems_dir.iterdir() if p.is_dir()])\n",
    "\n",
    "    for pid_dir in pid_dirs:\n",
    "        pid = pid_dir.name\n",
    "        buggy_base = pid_dir / \"buggy_programs\"\n",
    "        if not buggy_base.exists():\n",
    "            print(f\"[Warning] No buggy_programs for {pid}\")\n",
    "            continue\n",
    "\n",
    "        if lang:\n",
    "            selected_lang_dir = buggy_base / lang\n",
    "            if not selected_lang_dir.exists():\n",
    "                print(f\"[Warning] No buggy_programs/{lang} for {pid}\")\n",
    "                continue\n",
    "        else:\n",
    "            lang_dirs = [d for d in buggy_base.iterdir() if d.is_dir()]\n",
    "            if not lang_dirs:\n",
    "                print(f\"[Warning] No language directories in buggy_programs for {pid}\")\n",
    "                continue\n",
    "            selected_lang_dir = lang_dirs[0]\n",
    "            lang = selected_lang_dir.name\n",
    "\n",
    "        source_files = list(selected_lang_dir.glob(f\"*.{lang}\"))\n",
    "        if not source_files:\n",
    "            print(f\"[Warning] No source files found in: {selected_lang_dir}\")\n",
    "            continue\n",
    "\n",
    "        put_path = source_files[0]\n",
    "        put_code = _read_text_if_exists(put_path)\n",
    "        if not put_code:\n",
    "            continue\n",
    "\n",
    "        spec_path = pid_dir / \"problem_description.txt\"\n",
    "        spec_text = _read_text_if_exists(spec_path)\n",
    "\n",
    "        meta_path = pid_dir / \"metainfo.json\"\n",
    "        meta_data = {}\n",
    "        if meta_path.exists():\n",
    "            try:\n",
    "                meta_data = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Failed to read {meta_path}: {e}\")\n",
    "\n",
    "        loaded_problems[pid] = {\n",
    "            \"language\": lang,\n",
    "            \"spec\": spec_text,\n",
    "            \"put_code\": put_code,\n",
    "            \"meta\": {\n",
    "                \"pid_path\": str(pid_dir),\n",
    "                \"put_path\": str(put_path),\n",
    "                \"metainfo\": meta_data\n",
    "            }\n",
    "        }\n",
    "\n",
    "    print(f\"âœ… TrickyBugs ë¡œë”© ì™„ë£Œ: {len(loaded_problems)}ê°œ ë¬¸ì œ\")\n",
    "    return loaded_problems\n",
    "\n",
    "def load_quixbugs_data_revised(\n",
    "    extracted_code: Dict[str, Dict[str, str]]\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    QuixBugs ë°ì´í„° êµ¬ì¡°ì— ë§ì¶° ë¬¸ì œë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    extracted_code:\n",
    "        extract_python_code_from_dirs() ì˜ ë°˜í™˜ê°’.\n",
    "        {\n",
    "          \"python_programs\": { \"<name>.py\": \"...\", ... },\n",
    "          \"python_testcases\": { \"<name>_TEST.py\": \"...\", ... }\n",
    "        }\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loaded_problems:\n",
    "        {\n",
    "          \"<pid>\": {\n",
    "              \"language\": \"python\",\n",
    "              \"buggy_code\": <str>,\n",
    "              \"test_code\": <str or None>,\n",
    "              \"meta\": {\n",
    "                  \"program_filename\": <str>,\n",
    "                  \"test_filename\": <str or None>,\n",
    "              }\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    print(\"Loading QuixBugs data...\")\n",
    "\n",
    "    python_programs = extracted_code.get(\"python_programs\", {})\n",
    "    python_testcases = extracted_code.get(\"python_testcases\", {})\n",
    "\n",
    "    if not python_programs:\n",
    "        raise RuntimeError(\n",
    "            \"python_programsê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. \"\n",
    "            \"ë¨¼ì € extract_python_code_from_dirs()ë¥¼ ì‹¤í–‰í•´ì„œ extracted_codeë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\"\n",
    "        )\n",
    "\n",
    "    loaded_problems: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    # ê° .py í”„ë¡œê·¸ë¨ íŒŒì¼ì„ í•˜ë‚˜ì˜ problemìœ¼ë¡œ ê°„ì£¼\n",
    "    for prog_filename, prog_code in sorted(python_programs.items()):\n",
    "        if not prog_code:\n",
    "            continue\n",
    "        if prog_filename.endswith(\"_test.py\"):\n",
    "            continue\n",
    "\n",
    "        # pid: íŒŒì¼ëª…ì—ì„œ .py ì œê±° (ì˜ˆ: \"BINARY_SEARCH.py\" -> \"BINARY_SEARCH\")\n",
    "        pid, _ = os.path.splitext(prog_filename)\n",
    "\n",
    "        # ëŒ€ì‘ë˜ëŠ” í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°\n",
    "        # ëŒ€í‘œì ì¸ íŒ¨í„´: <PID>_TEST.py\n",
    "        candidate_names = [\n",
    "            #f\"{pid}_TEST.py\",\n",
    "            #f\"{pid}_test.py\",\n",
    "            #f\"TEST_{pid}.py\",\n",
    "            f\"test_{pid}.py\",\n",
    "        ]\n",
    "\n",
    "        test_code = None\n",
    "        test_filename = None\n",
    "\n",
    "        for cand in candidate_names:\n",
    "            if cand in python_testcases:\n",
    "                test_filename = cand\n",
    "                test_code = python_testcases[cand]\n",
    "                break\n",
    "\n",
    "        # í˜¹ì‹œ ì´ë¦„ íŒ¨í„´ì´ ë‹¬ë¼ì„œ ëª» ì°¾ì€ ê²½ìš°: pidê°€ í¬í•¨ëœ ì²« í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ fallbackìœ¼ë¡œ ì‚¬ìš©\n",
    "        if test_code is None:\n",
    "            for fname, content in python_testcases.items():\n",
    "                base_no_ext, _ = os.path.splitext(fname)\n",
    "                if pid.lower() in base_no_ext.lower():\n",
    "                    test_filename = fname\n",
    "                    test_code = content\n",
    "                    break\n",
    "\n",
    "        loaded_problems[pid] = {\n",
    "            \"language\": \"python\",\n",
    "            \"buggy_code\": prog_code,\n",
    "            \"test_code\": test_code,\n",
    "            \"meta\": {\n",
    "                \"program_filename\": prog_filename,\n",
    "                \"test_filename\": test_filename,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    print(f\"âœ… QuixBugs ë¡œë”© ì™„ë£Œ: {len(loaded_problems)}ê°œ ë¬¸ì œ\")\n",
    "    return loaded_problems\n",
    "\n",
    "def create_variants(problems, output_base_path, k: int = 1):\n",
    "    \"\"\"\n",
    "    TrickyBugs ë¬¸ì œ ë°ì´í„°ì—ì„œ ê° ì½”ë“œì˜ ìˆ˜ì • ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    genprog_prompt = \"\"\"You are a professional coding competition participant, skilled at identifying bugs and logic flaws in code.\n",
    "You will receive a description of a coding problem, and a piece of code attempting to solve the problem.\n",
    "Your task is to repair the code.\n",
    "\n",
    "Your response MUST contain only the complete C++ code, formatted inside a single markdown code block.\n",
    "Start your response IMMEDIATELY with ```cpp and end your response IMMEDIATELY with ```.\n",
    "Do not provide any text, explanation, or reasoning before or after the code block.\n",
    "\n",
    "**PROBLEM DESCRIPTION**:\n",
    "{pro_des}\n",
    "\n",
    "**CODE**:\n",
    "{code}\n",
    "\"\"\"\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        print(f\"\\n--- Processing PID: {pid} ---\")\n",
    "        variant_dir = Path(output_base_path) / \"GenProgs\" / pid\n",
    "        variant_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(1, k + 1):\n",
    "            print(f\"ğŸ¤– Generating repaired program variant #{i}...\")\n",
    "\n",
    "            prog_messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": genprog_prompt.format(\n",
    "                        pro_des=problem[\"spec\"],\n",
    "                        code=problem[\"put_code\"]\n",
    "                    )\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # âœ… ìˆ˜ì •: generate_code_response ì§ì ‘ í˜¸ì¶œ, 1280 í† í° ì‚¬ìš©\n",
    "            variant_code = generate_code_response(prog_messages, max_new_tokens=1280)\n",
    "\n",
    "            variant_file = variant_dir / f\"variant_{i}.{problem['language']}\"\n",
    "            with open(variant_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(variant_code)\n",
    "\n",
    "            print(f\"   âœ… Saved repaired code to: {variant_file}\")\n",
    "\n",
    "    print(f\"\\nğŸ¯ ëª¨ë“  ë¬¸ì œì˜ {k}ê°œ ë³€í˜• ì½”ë“œ ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "def create_quixbugs_variants(problems: Dict[str, Dict[str, Any]],\n",
    "                             output_base_path: str,\n",
    "                             k: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    QuixBugs ë¬¸ì œ ë°ì´í„°ì—ì„œ ê° ì½”ë“œì˜ ìˆ˜ì • ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problems : dict\n",
    "        load_quixbugs_data_revised() ì—ì„œ ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬.\n",
    "        ì˜ˆì‹œ êµ¬ì¡°:\n",
    "        {\n",
    "          \"BINARY_SEARCH\": {\n",
    "              \"language\": \"python\",\n",
    "              \"buggy_code\": \"...\",\n",
    "              \"test_code\": \"...\",   # ì—†ì„ ìˆ˜ë„ ìˆìŒ\n",
    "              \"meta\": {\n",
    "                  \"program_filename\": \"BINARY_SEARCH.py\",\n",
    "                  \"test_filename\": \"BINARY_SEARCH_TEST.py\",\n",
    "              }\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    output_base_path : str\n",
    "        ë³€í˜• ì½”ë“œë¥¼ ì €ì¥í•  ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ.\n",
    "        ì‹¤ì œ ì €ì¥ ê²½ë¡œëŠ” {output_base_path}/GenProgs/{pid}/variant_i.py ê°€ ë¨.\n",
    "\n",
    "    k : int\n",
    "        ê° ë¬¸ì œë‹¹ ìƒì„±í•  ë³€í˜•(ìˆ˜ì •) ì½”ë“œì˜ ê°œìˆ˜.\n",
    "    \"\"\"\n",
    "\n",
    "    genprog_prompt = \"\"\"You are a professional software engineer skilled at identifying bugs and logic flaws in Python code.\n",
    "You will receive a description of a coding problem (if available), and a buggy Python implementation attempting to solve it.\n",
    "Your task is to repair the code so that it is correct and robust.\n",
    "\n",
    "Your response MUST contain only the complete Python code, formatted inside a single markdown code block.\n",
    "Start your response IMMEDIATELY with ```python and end your response IMMEDIATELY with ```.\n",
    "Do not provide any text, explanation, or reasoning before or after the code block.\n",
    "\n",
    "If there are multiple functions or helper code in the original file, keep the overall file structure but fix the bugs.\n",
    "\n",
    "**PROBLEM DESCRIPTION (may be minimal)**:\n",
    "{pro_des}\n",
    "\n",
    "**BUGGY PYTHON CODE**:\n",
    "{code}\n",
    "\"\"\"\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        print(f\"\\n--- Processing QuixBugs PID: {pid} ---\")\n",
    "\n",
    "        buggy_code = problem.get(\"buggy_code\")\n",
    "        if not buggy_code:\n",
    "            print(f\"[Warning] No 'buggy_code' for pid={pid}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # QuixBugsì—ëŠ” ë³„ë„ì˜ ìì—°ì–´ ìŠ¤í™ì´ ì—†ìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ë§Œë“¤ì–´ ì¤€ë‹¤.\n",
    "        spec = (\n",
    "            f\"This is a buggy Python implementation from the QuixBugs benchmark for problem '{pid}'. \"\n",
    "            f\"Fix all bugs so that it passes the corresponding unit tests.\"\n",
    "        )\n",
    "\n",
    "        variant_dir = Path(output_base_path) / \"GenProgs\" / pid\n",
    "        variant_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(1, k + 1):\n",
    "            print(f\"ğŸ¤– Generating repaired program variant #{i} for {pid}...\")\n",
    "\n",
    "            prompt_text = genprog_prompt.format(\n",
    "                pro_des=spec,\n",
    "                code=buggy_code,\n",
    "            )\n",
    "\n",
    "            prog_messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_text,\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆëŠ” í•¨ìˆ˜ë¼ê³  ê°€ì •\n",
    "            variant_code = generate_code_response(\n",
    "                prog_messages,\n",
    "                max_new_tokens=1280,\n",
    "            )\n",
    "\n",
    "            variant_file = variant_dir / f\"variant_{i}.py\"\n",
    "            with open(variant_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(variant_code)\n",
    "\n",
    "            print(f\"   âœ… Saved repaired code to: {variant_file}\")\n",
    "\n",
    "    print(f\"\\nğŸ¯ QuixBugsì˜ ëª¨ë“  ë¬¸ì œì— ëŒ€í•´ ê° {k}ê°œ ë³€í˜• ì½”ë“œ ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "def generate_buggy_test_inputs(problems, output_base_path, num_inputs: int = 10):\n",
    "    \"\"\"\n",
    "    LLMì—ê²Œ ê° ë¬¸ì œ ëª…ì„¸ì™€ ë²„ê·¸ ì½”ë“œë¥¼ ì „ë‹¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    output_base = Path(output_base_path) / \"chat_generated_inputs\"\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        print(f\"\\n--- Generating test inputs for PID: {pid} ---\")\n",
    "        \n",
    "        pid_dir = output_base / pid\n",
    "        pid_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        prompt = f\"\"\"**INSTRUCTION**:\n",
    "You are a professional software testing engineer. You will get a problem description of a coding problem, and a piece of code attempting to solve the problem. \n",
    "Please generate {num_inputs} diverse and corner test inputs that could potentially trigger bugs.\n",
    "Every input must adhere to the constraints and format mentioned in the problem description.\n",
    "Please reply with ONLY the generated input without any other content, use the following template:\n",
    "INPUT1:\n",
    "(content of the 1st generated test input)\n",
    "INPUT2:\n",
    "(content of the 2nd generated test input)\n",
    "...\n",
    "INPUT{num_inputs}:\n",
    "(content of the {num_inputs}-th generated test input)\n",
    "\n",
    "**PROBLEM DESCRIPTION**:\n",
    "{problem[\"spec\"]}\n",
    "\n",
    "**CODE**:\n",
    "{problem[\"put_code\"]}\n",
    "\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        # âœ… ìˆ˜ì •: generate_text_response ì§ì ‘ í˜¸ì¶œ, 512 í† í° ì‚¬ìš©\n",
    "        response = generate_text_response(messages, max_new_tokens=512)\n",
    "\n",
    "        pattern = r\"INPUT\\d+:\\s*(.*?)\\s*(?=INPUT\\d+:|$)\"\n",
    "        matches = re.findall(pattern, response, flags=re.DOTALL)\n",
    "\n",
    "        if not matches:\n",
    "            print(f\"  âŒ No inputs generated for PID {pid}\")\n",
    "            continue\n",
    "\n",
    "        for idx, input_content in enumerate(matches[:num_inputs], start=1):\n",
    "            input_file = pid_dir / f\"chatGenInput_{idx}.in\"\n",
    "            input_file.write_text(input_content.strip(), encoding=\"utf-8\")\n",
    "            print(f\"  âœ… Saved {input_file.name}\")\n",
    "\n",
    "    print(\"\\nğŸ¯ ëª¨ë“  ë¬¸ì œì˜ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "def generate_buggy_test_inputs_quixbugs(\n",
    "    problems: Dict[str, Dict[str, Any]],\n",
    "    output_base_path: str,\n",
    "    num_inputs: int = 10,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    QuixBugs ë¬¸ì œë“¤ì— ëŒ€í•´, LLMì—ê²Œ ë²„ê·¸ê°€ ìˆëŠ” Python ì½”ë“œ(ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ)ë¥¼ ì „ë‹¬í•˜ì—¬\n",
    "    ì ì¬ì  ë²„ê·¸ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆëŠ” í…ŒìŠ¤íŠ¸ ì…ë ¥ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problems : dict\n",
    "        load_quixbugs_data_revised() ì—ì„œ ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬.\n",
    "        ì˜ˆì‹œ êµ¬ì¡°:\n",
    "        {\n",
    "          \"BINARY_SEARCH\": {\n",
    "              \"language\": \"python\",\n",
    "              \"buggy_code\": \"...\",\n",
    "              \"test_code\": \"...\",   # ì—†ì„ ìˆ˜ë„ ìˆìŒ\n",
    "              \"meta\": {\n",
    "                  \"program_filename\": \"BINARY_SEARCH.py\",\n",
    "                  \"test_filename\": \"BINARY_SEARCH_TEST.py\",\n",
    "              }\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    output_base_path : str\n",
    "        ìƒì„±ëœ ì…ë ¥ íŒŒì¼ì„ ì €ì¥í•  ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œ.\n",
    "        ì‹¤ì œ ì €ì¥ ê²½ë¡œëŠ” {output_base_path}/chat_generated_inputs/{pid}/chatGenInput_i.in\n",
    "\n",
    "    num_inputs : int\n",
    "        ê° ë¬¸ì œë‹¹ ìƒì„±í•  ì…ë ¥ ê°œìˆ˜.\n",
    "    \"\"\"\n",
    "\n",
    "    output_base = Path(output_base_path) / \"chat_generated_inputs\"\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        print(f\"\\n--- Generating test inputs for QuixBugs PID: {pid} ---\")\n",
    "\n",
    "        buggy_code = problem.get(\"buggy_code\")\n",
    "        if not buggy_code:\n",
    "            print(f\"  âŒ No 'buggy_code' for PID {pid}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        test_code = problem.get(\"test_code\")\n",
    "        if test_code:\n",
    "            code_block_for_prompt = f\"{buggy_code}\\n\\n# === UNIT TESTS (QuixBugs) ===\\n{test_code}\"\n",
    "        else:\n",
    "            code_block_for_prompt = buggy_code\n",
    "\n",
    "        # QuixBugsëŠ” ë³„ë„ì˜ ìì—°ì–´ ìŠ¤í™ì´ ì—†ìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ìƒì„±\n",
    "        spec = (\n",
    "            f\"This is a buggy Python implementation from the QuixBugs benchmark for problem '{pid}'. \"\n",
    "            f\"The code may contain logical errors or corner cases that are not handled correctly. \"\n",
    "            f\"Use the function signature and unit tests (if provided) to infer valid input formats.\"\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"**INSTRUCTION**:\n",
    "You are a professional software testing engineer. You will be given a buggy Python implementation of a QuixBugs benchmark problem, and optionally some unit tests.\n",
    "Your task is to generate {num_inputs} diverse and corner-case test inputs that could potentially trigger remaining bugs.\n",
    "\n",
    "Every input must:\n",
    "- Respect the expected parameter types and formats inferred from the function signature and tests.\n",
    "- Cover edge cases, typical cases, and tricky cases (e.g., empty structures, large values, negative values, duplicates, sorted/unsorted variations, etc. as appropriate).\n",
    "\n",
    "Please reply with ONLY the generated inputs, using the following template exactly:\n",
    "\n",
    "INPUT1:\n",
    "(content of the 1st generated test input)\n",
    "INPUT2:\n",
    "(content of the 2nd generated test input)\n",
    "...\n",
    "INPUT{num_inputs}:\n",
    "(content of the {num_inputs}-th generated test input)\n",
    "\n",
    "You may assume that each INPUT corresponds to one invocation of the main function under test\n",
    "(e.g., a tuple/list of arguments, or a single value, depending on the problem).\n",
    "\n",
    "**PROBLEM CONTEXT**:\n",
    "{spec}\n",
    "\n",
    "**BUGGY PYTHON CODE (AND UNIT TESTS IF AVAILABLE)**:\n",
    "{code_block_for_prompt}\n",
    "\"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        # ì‚¬ìš©ìê°€ ì´ë¯¸ ì •ì˜í•´ë‘” LLM í˜¸ì¶œ í•¨ìˆ˜ë¼ê³  ê°€ì •\n",
    "        response = generate_text_response(messages, max_new_tokens=512)\n",
    "\n",
    "        # INPUT1: ... INPUT2: ... íŒ¨í„´ìœ¼ë¡œ íŒŒì‹±\n",
    "        pattern = r\"INPUT\\d+:\\s*(.*?)\\s*(?=INPUT\\d+:|$)\"\n",
    "        matches = re.findall(pattern, response, flags=re.DOTALL)\n",
    "\n",
    "        if not matches:\n",
    "            print(f\"  âŒ No inputs generated for PID {pid}\")\n",
    "            continue\n",
    "\n",
    "        pid_dir = output_base / pid\n",
    "        pid_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for idx, input_content in enumerate(matches[:num_inputs], start=1):\n",
    "            input_file = pid_dir / f\"chatGenInput_{idx}.in\"\n",
    "            input_file.write_text(input_content.strip(), encoding=\"utf-8\")\n",
    "            print(f\"  âœ… Saved {input_file.name}\")\n",
    "\n",
    "    print(\"\\nğŸ¯ QuixBugs ì „ì²´ ë¬¸ì œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„± ì™„ë£Œ!\")\n",
    "\n",
    "def verify_variants(problems, output_base_path):\n",
    "    \"\"\"\n",
    "    ìƒì„±ëœ variant ì½”ë“œ ì¤‘ ì›ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ëª¨ë‘ í†µê³¼í•˜ëŠ” ì½”ë“œë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    genprogs_base = Path(output_base_path) / \"GenProgs\"\n",
    "    verified_base = Path(output_base_path) / \"GenProgsVerified\"\n",
    "    genprogs_base.mkdir(parents=True, exist_ok=True)\n",
    "    verified_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        lang = problem[\"language\"]\n",
    "        print(f\"\\n--- Verifying PID: {pid} ({lang}) ---\")\n",
    "        \n",
    "        variant_dir = genprogs_base / pid\n",
    "        if not variant_dir.exists():\n",
    "            print(f\"[Warning] No variants found for {pid}\")\n",
    "            continue\n",
    "\n",
    "        test_dir = Path(problem[\"meta\"][\"pid_path\"]) / \"original_test_cases\"\n",
    "        if not test_dir.exists():\n",
    "            print(f\"[Warning] No original_test_cases for {pid}\")\n",
    "            continue\n",
    "\n",
    "        variant_files = list(variant_dir.glob(f\"*.{lang}\"))\n",
    "        for variant_file in variant_files:\n",
    "            print(f\"Checking {variant_file.name} ...\")\n",
    "            all_passed = True\n",
    "\n",
    "            if lang == \"cpp\":\n",
    "                exec_file = variant_file.parent / \"tmp_exec\"\n",
    "                try:\n",
    "                    compile_cmd = [\"g++\", \"-std=c++17\", str(variant_file), \"-o\", str(exec_file)]\n",
    "                    subprocess.run(compile_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"  âŒ Compile failed: {e}\")\n",
    "                    all_passed = False\n",
    "                    continue\n",
    "            else:\n",
    "                exec_file = variant_file\n",
    "\n",
    "            input_files = sorted(test_dir.glob(\"*.in\"))\n",
    "            for in_file in input_files:\n",
    "                out_file = test_dir / (in_file.stem + \".out\")\n",
    "                if not out_file.exists():\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if lang == \"cpp\":\n",
    "                        result = subprocess.run([str(exec_file)], input=in_file.read_bytes(),\n",
    "                                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)\n",
    "                    else:\n",
    "                        result = subprocess.run([\"python3\", str(exec_file)], input=in_file.read_bytes(),\n",
    "                                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=5)\n",
    "\n",
    "                    expected_output = out_file.read_bytes()\n",
    "                    if result.stdout.strip() != expected_output.strip():\n",
    "                        all_passed = False\n",
    "                        print(f\"  âŒ Test failed: {in_file.name}\")\n",
    "                        break\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    all_passed = False\n",
    "                    print(f\"  âŒ Test timed out: {in_file.name}\")\n",
    "                    break\n",
    "\n",
    "            if all_passed:\n",
    "                pid_verified_dir = verified_base / pid\n",
    "                pid_verified_dir.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy(variant_file, pid_verified_dir / variant_file.name)\n",
    "                print(f\"  âœ… Passed all tests: {variant_file.name}\")\n",
    "\n",
    "            if lang == \"cpp\" and exec_file.exists():\n",
    "                exec_file.unlink()\n",
    "\n",
    "    print(\"\\nğŸ¯ ëª¨ë“  ë¬¸ì œì˜ Verified variants ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def verify_quixbugs_variants(\n",
    "    problems: Dict[str, Dict[str, Any]],\n",
    "    output_base_path: str,\n",
    "    timeout_sec: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    QuixBugsìš© ë³€ì´ ì½”ë“œ ê²€ì¦ í•¨ìˆ˜.\n",
    "    - GenProgs/{pid}/ ì•„ë˜ì— ìƒì„±ëœ variant_*.py ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ\n",
    "    - QuixBugs ìŠ¤íƒ€ì¼ pytest í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•œ variantë§Œ\n",
    "      GenProgsVerified/{pid}/ ë¡œ ë³µì‚¬í•œë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    genprogs_base = Path(output_base_path) / \"GenProgs\"\n",
    "    verified_base = Path(output_base_path) / \"GenProgsVerified\"\n",
    "    genprogs_base.mkdir(parents=True, exist_ok=True)\n",
    "    verified_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        lang = problem.get(\"language\", \"python\")\n",
    "        print(f\"\\n--- Verifying QuixBugs PID: {pid} ({lang}) ---\")\n",
    "\n",
    "        if lang != \"python\":\n",
    "            print(f\"[Warning] Language {lang} is not supported (expected 'python'). Skipping {pid}.\")\n",
    "            continue\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ì½”ë“œ ë° íŒŒì¼ ì´ë¦„ ì •ë³´ í™•ì¸\n",
    "        test_code = problem.get(\"test_code\")\n",
    "        meta = problem.get(\"meta\", {})\n",
    "        prog_filename = meta.get(\"program_filename\")\n",
    "        test_filename = meta.get(\"test_filename\")\n",
    "\n",
    "        if not test_code or not prog_filename or not test_filename:\n",
    "            print(f\"[Warning] Missing test_code or filenames for {pid}. Skipping verification.\")\n",
    "            continue\n",
    "\n",
    "        variant_dir = genprogs_base / pid\n",
    "        if not variant_dir.exists():\n",
    "            print(f\"[Warning] No variants found for {pid} at {variant_dir}\")\n",
    "            continue\n",
    "\n",
    "        variant_files = sorted(variant_dir.glob(\"*.py\"))\n",
    "        if not variant_files:\n",
    "            print(f\"[Warning] No .py variant files in {variant_dir}\")\n",
    "            continue\n",
    "\n",
    "        for variant_file in variant_files:\n",
    "            print(f\"Checking {variant_file.name} ...\")\n",
    "\n",
    "            all_passed = False  # ê¸°ë³¸ì€ ì‹¤íŒ¨ë¡œ ë‘ê³ , í…ŒìŠ¤íŠ¸ í†µê³¼ ì‹œì—ë§Œ True\n",
    "\n",
    "            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì•ˆì— QuixBugs êµ¬ì¡°ë¥¼ ì¬í˜„í•´ì„œ pytest ì‹¤í–‰\n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                tmp_root = Path(tmpdir)\n",
    "\n",
    "                # python_programs/ ë° python_testcases/ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "                prog_dir = tmp_root / \"python_programs\"\n",
    "                test_dir = tmp_root / \"python_testcases\"\n",
    "                prog_dir.mkdir(parents=True, exist_ok=True)\n",
    "                test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # íŒ¨í‚¤ì§€ ì¸ì‹ë˜ë„ë¡ __init__.py ì¶”ê°€\n",
    "                (prog_dir / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "                (test_dir / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "                # 1) ë³€ì´ ì½”ë“œë¥¼ python_programs/{program_filename} ìœ¼ë¡œ ë³µì‚¬\n",
    "                variant_code = variant_file.read_text(encoding=\"utf-8\")\n",
    "                (prog_dir / prog_filename).write_text(variant_code, encoding=\"utf-8\")\n",
    "\n",
    "                # 2) ì›ë³¸ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ python_testcases/{test_filename} ìœ¼ë¡œ ì €ì¥\n",
    "                (test_dir / test_filename).write_text(test_code, encoding=\"utf-8\")\n",
    "\n",
    "                # 3) QuixBugs ì¸í”„ë¼ íŒŒì¼ë“¤ ë³µì‚¬\n",
    "                root = Path(QUIXBUGS_ROOT_DIR)  # ë¬¸ìì—´ì´ë©´ Pathë¡œ ê°ì‹¸ê¸°\n",
    "\n",
    "                # 3-1) conftest.py (pytest.use_correct í”Œë˜ê·¸ ì •ì˜ìš©)\n",
    "                src_conftest = root / \"conftest.py\"\n",
    "                if src_conftest.exists():\n",
    "                    shutil.copy(src_conftest, tmp_root / \"conftest.py\")\n",
    "\n",
    "                # 3-2) load_testdata.py\n",
    "                src_load = root / \"python_testcases\" / \"load_testdata.py\"\n",
    "                if src_load.exists():\n",
    "                    shutil.copy(src_load, test_dir / \"load_testdata.py\")\n",
    "\n",
    "                # 3-3) json_testcases ë””ë ‰í† ë¦¬ ì „ì²´\n",
    "                src_json = root / \"json_testcases\"\n",
    "                dst_json = tmp_root / \"json_testcases\"\n",
    "                if src_json.exists():\n",
    "                    shutil.copytree(src_json, dst_json)\n",
    "\n",
    "                # 3-4) node.py ê°™ì€ ê³µí†µ ëª¨ë“ˆ (ê·¸ë˜í”„ ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ìš©)\n",
    "                src_node = root / \"python_programs\" / \"node.py\"\n",
    "                if src_node.exists():\n",
    "                    shutil.copy(src_node, prog_dir / \"node.py\")\n",
    "\n",
    "                # 4) pytestë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "                test_path = test_dir / test_filename\n",
    "                try:\n",
    "                    # cwd ë¥¼ tmp_root ë¡œ ë‘ë©´, í…ŒìŠ¤íŠ¸ ì½”ë“œì˜\n",
    "                    #   from python_programs.BINARY_SEARCH import ...\n",
    "                    #   from load_testdata import load_json_testcases\n",
    "                    # ë“±ì´ ì •ìƒ ë™ì‘í•¨\n",
    "                    result = subprocess.run(\n",
    "                        [sys.executable, \"-m\", \"pytest\", str(test_path)],\n",
    "                        cwd=str(tmp_root),\n",
    "                        stdout=subprocess.PIPE,\n",
    "                        stderr=subprocess.PIPE,\n",
    "                        timeout=timeout_sec,\n",
    "                    )\n",
    "\n",
    "                    if result.returncode == 0:\n",
    "                        all_passed = True\n",
    "                    else:\n",
    "                        print(f\"  âŒ Tests failed for {variant_file.name}\")\n",
    "                        # ë””ë²„ê¹… í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "                        # print(result.stdout.decode(\"utf-8\", errors=\"ignore\"))\n",
    "                        # print(result.stderr.decode(\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    print(f\"  âŒ Test timed out for {variant_file.name} (>{timeout_sec}s)\")\n",
    "\n",
    "            # ì„ì‹œ ë””ë ‰í† ë¦¬ context ë¥¼ ë²—ì–´ë‚˜ë©´ ìë™ ì‚­ì œë¨\n",
    "\n",
    "            if all_passed:\n",
    "                pid_verified_dir = verified_base / pid\n",
    "                pid_verified_dir.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy(variant_file, pid_verified_dir / variant_file.name)\n",
    "                print(f\"  âœ… Passed all tests: {variant_file.name}\")\n",
    "\n",
    "    print(\"\\nğŸ¯ QuixBugs ëª¨ë“  ë¬¸ì œì— ëŒ€í•œ Verified variants ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "\n",
    "def run_code(lang:str, code_path:Path, input_bytes:bytes, timeout:int=5) -> bytes:\n",
    "    \"\"\"C++ ë˜ëŠ” Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  stdout ë°˜í™˜\"\"\"\n",
    "    if lang == \"cpp\":\n",
    "        exec_file = code_path.parent / \"tmp_exec\"\n",
    "        try:\n",
    "            subprocess.run([\"g++\", \"-std=c++17\", str(code_path), \"-o\", str(exec_file)],\n",
    "                           check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"[Compile Error] {code_path}: {e}\")\n",
    "            return b\"\"\n",
    "        try:\n",
    "            result = subprocess.run([str(exec_file)], input=input_bytes,\n",
    "                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return b\"TIMEOUT\"\n",
    "        finally:\n",
    "            if exec_file.exists():\n",
    "                exec_file.unlink()\n",
    "        return result.stdout\n",
    "    elif lang == \"python\":\n",
    "        try:\n",
    "            result = subprocess.run([\"python3\", str(code_path)], input=input_bytes,\n",
    "                                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return b\"TIMEOUT\"\n",
    "        return result.stdout\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {lang}\")\n",
    "\n",
    "from typing import Tuple\n",
    "def run_quixbugs_tests(\n",
    "    test_path: Path,\n",
    "    cwd: Path,\n",
    "    timeout: int = 5,\n",
    ") -> Tuple[int, bytes, bytes]:\n",
    "    \"\"\"\n",
    "    QuixBugs ìŠ¤íƒ€ì¼ì˜ Python í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³ ,\n",
    "    (returncode, stdout, stderr)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_path : Path\n",
    "        ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ (ì˜ˆ: tmp_root / \"python_testcases\" / \"BINARY_SEARCH_TEST.py\")\n",
    "    cwd : Path\n",
    "        ì‹¤í–‰ ì‹œ ê¸°ì¤€ì´ ë  ì‘ì—… ë””ë ‰í† ë¦¬.\n",
    "        ë³´í†µ QuixBugs êµ¬ì¡°( python_programs/, python_testcases/ )ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬.\n",
    "    timeout : int\n",
    "        ì´ˆ ë‹¨ìœ„ íƒ€ì„ì•„ì›ƒ.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returncode : int\n",
    "        0 ì´ë©´ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼, 0ì´ ì•„ë‹ˆë©´ ì‹¤íŒ¨/ì˜ˆì™¸.\n",
    "    stdout : bytes\n",
    "        í‘œì¤€ ì¶œë ¥.\n",
    "    stderr : bytes\n",
    "        í‘œì¤€ ì—ëŸ¬ ì¶œë ¥.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", str(test_path)],\n",
    "            cwd=str(cwd),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        # íƒ€ì„ì•„ì›ƒ ì‹œ, returncodeë¥¼ ë³„ë„ë¡œ í‘œì‹œ(ì˜ˆ: -1)í•˜ê³  stderrì— ë©”ì‹œì§€ ë„£ê¸°\n",
    "        return -1, b\"\", b\"TIMEOUT\"\n",
    "\n",
    "    return result.returncode, result.stdout, result.stderr\n",
    "\n",
    "def task_oracle(problems:Dict, inputs_base_path:str, lang:str):\n",
    "    \"\"\"\n",
    "    ê° ë¬¸ì œì˜ PUTê³¼ reference ì†”ë£¨ì…˜ì— ëŒ€í•´ LLMì´ ìƒì„±í•œ í…ŒìŠ¤íŠ¸ ì…ë ¥ì„ ì‹¤í–‰í•˜ê³ \n",
    "    ì¶œë ¥ ë¹„êµ í›„ íŠ¸ë¦¬ê±° ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame(columns=['pid', 'input_name', 'sol_name', 'out', 'input_valid', 'out_correct'])\n",
    "    \"\"\"\n",
    "    df_rows = []\n",
    "    inputs_base = Path(inputs_base_path)\n",
    "    \n",
    "    for pid, problem in problems.items():\n",
    "        pid_inputs_dir = inputs_base / pid\n",
    "        if not pid_inputs_dir.exists():\n",
    "            print(f\"[Warning] No test inputs for {pid}\")\n",
    "            continue\n",
    "        input_files = sorted(pid_inputs_dir.glob(\"*.in\"))\n",
    "        \n",
    "        # ëª¨ë“  ì†”ë£¨ì…˜ (PUT + other) ê²½ë¡œ ìˆ˜ì§‘\n",
    "        sol_files = [(problem['put_code'], 'put')]\n",
    "        # GenProgsVerifiedê°€ ìˆë‹¤ë©´ reference ì†”ë£¨ì…˜ ì¶”ê°€\n",
    "        verified_dir = Path(inputs_base_path).parent / \"GenProgsVerified\" / pid\n",
    "        if verified_dir.exists():\n",
    "            for f in verified_dir.glob(f\"*.{lang}\"):\n",
    "                sol_files.append((f, f.stem))\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            input_bytes = input_file.read_bytes()\n",
    "            for code, sol_name in sol_files:\n",
    "                if isinstance(code, str):\n",
    "                    # PUT ì½”ë“œëŠ” ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "                    tmp_file = Path(f\"tmp_put_{pid}.{lang}\")\n",
    "                    tmp_file.write_text(code, encoding=\"utf-8\")\n",
    "                    code_path = tmp_file\n",
    "                else:\n",
    "                    code_path = code  # ì´ë¯¸ íŒŒì¼ì¸ reference\n",
    "                out = run_code(lang, code_path, input_bytes)\n",
    "                if isinstance(code, str) and tmp_file.exists():\n",
    "                    tmp_file.unlink()\n",
    "                input_valid = out not in [b\"\", b\"TIMEOUT\"]\n",
    "                df_rows.append({\n",
    "                    \"pid\": pid,\n",
    "                    \"input_name\": input_file.name,\n",
    "                    \"sol_name\": sol_name,\n",
    "                    \"out\": out,\n",
    "                    \"input_valid\": input_valid,\n",
    "                    \"out_correct\": input_valid  # PUT ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(df_rows)\n",
    "    return df\n",
    "\n",
    "from typing import Union\n",
    "def _run_quixbugs_on_input(\n",
    "    pid: str,\n",
    "    problem: Dict[str, Any],\n",
    "    code: Union[str, Path],\n",
    "    sol_name: str,\n",
    "    input_bytes: bytes,\n",
    "    timeout: int = 5,\n",
    ") -> bytes:\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ QuixBugs ë¬¸ì œ + ë‹¨ì¼ ì†”ë£¨ì…˜(code) + ë‹¨ì¼ ì…ë ¥(input_bytes)ì— ëŒ€í•´\n",
    "    í•¨ìˆ˜ë¥¼ í•œ ë²ˆ ì‹¤í–‰í•˜ê³  stdout(bytes)ì„ ë°˜í™˜í•œë‹¤.\n",
    "    .in íŒŒì¼ ë‚´ìš©ì€ Python literal ë¡œ ê°„ì£¼í•˜ì—¬ ast.literal_eval ë¡œ íŒŒì‹±.\n",
    "\n",
    "    - program_filename: e.g. \"BINARY_SEARCH.py\"\n",
    "    - í•¨ìˆ˜ëª…: program_filenameì˜ stemì„ ì†Œë¬¸ìë¡œ ë°”ê¾¼ ê²ƒ (ì˜ˆ: \"BINARY_SEARCH\" -> \"binary_search\")\n",
    "    \"\"\"\n",
    "    meta = problem.get(\"meta\", {})\n",
    "    prog_filename = meta.get(\"program_filename\")\n",
    "    if not prog_filename:\n",
    "        print(f\"[Warning] No program_filename in meta for pid={pid}\")\n",
    "        return b\"\"\n",
    "\n",
    "    module_name = Path(prog_filename).stem          # \"BINARY_SEARCH\"\n",
    "    func_name = module_name.lower()                # \"binary_search\"\n",
    "\n",
    "    try:\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            tmp_root = Path(tmpdir)\n",
    "            prog_dir = tmp_root / \"python_programs\"\n",
    "            prog_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # íŒ¨í‚¤ì§€ ì¸ì‹\n",
    "            (prog_dir / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "            # 1) ì†”ë£¨ì…˜ ì½”ë“œë¥¼ python_programs/{prog_filename} ìœ¼ë¡œ ì €ì¥\n",
    "            target_prog_path = prog_dir / prog_filename\n",
    "            if isinstance(code, str):\n",
    "                target_prog_path.write_text(code, encoding=\"utf-8\")\n",
    "            else:\n",
    "                # Path ì¸ ê²½ìš° íŒŒì¼ ë³µì‚¬\n",
    "                shutil.copy(code, target_prog_path)\n",
    "\n",
    "            # 2) run_main.py ë˜í¼ ìƒì„±\n",
    "            wrapper_path = tmp_root / \"run_main.py\"\n",
    "            wrapper_src = f\"\"\"import sys, ast\n",
    "from python_programs.{module_name} import {func_name}\n",
    "\n",
    "def main():\n",
    "    s = sys.stdin.read().strip()\n",
    "    if not s:\n",
    "        args = ()\n",
    "    else:\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            # ì…ë ¥ íŒŒì‹± ì‹¤íŒ¨\n",
    "            print(\"INPUT_PARSE_ERROR\", end=\"\")\n",
    "            return\n",
    "        if isinstance(obj, (tuple, list)):\n",
    "            args = tuple(obj)\n",
    "        else:\n",
    "            args = (obj,)\n",
    "\n",
    "    try:\n",
    "        res = {func_name}(*args)\n",
    "    except Exception as e:\n",
    "        # í•¨ìˆ˜ ë‚´ë¶€ ì—ëŸ¬ -> ì‹¤íŒ¨ë¡œ ê°„ì£¼\n",
    "        print(\"RUNTIME_ERROR\", end=\"\")\n",
    "        return\n",
    "\n",
    "    # repr ë¡œ ì¶œë ¥í•˜ë©´ ë¦¬ìŠ¤íŠ¸/íŠœí”Œ/ìˆ«ì ë“± ë¹„êµê°€ ì¼ì •í•˜ê²Œ ê°€ëŠ¥\n",
    "    sys.stdout.write(repr(res))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "            wrapper_path.write_text(wrapper_src, encoding=\"utf-8\")\n",
    "\n",
    "            # 3) ì‹¤í–‰\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python\", str(wrapper_path)],\n",
    "                    input=input_bytes,\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,\n",
    "                    timeout=timeout,\n",
    "                    cwd=str(tmp_root),\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(f\"[Timeout] pid={pid}, sol={sol_name}\")\n",
    "                return b\"TIMEOUT\"\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                # í…ŒìŠ¤íŠ¸/í•¨ìˆ˜ ë‚´ë¶€ ì˜ˆì™¸ ë“±\n",
    "                # stderrë¥¼ ë³´ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì„œ decode í•´ì„œ ì¶œë ¥ ê°€ëŠ¥\n",
    "                return result.stdout or b\"\"\n",
    "\n",
    "            return result.stdout\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] pid={pid}, sol={sol_name}: {e}\")\n",
    "        return b\"\"\n",
    "\n",
    "\n",
    "def task_oracle_quixbugs(\n",
    "    problems: Dict[str, Dict[str, Any]],\n",
    "    inputs_base_path: str,\n",
    "    timeout: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    QuixBugs ì „ìš© task_oracle:\n",
    "    - ê° ë¬¸ì œì— ëŒ€í•´ LLMì´ ìƒì„±í•œ .in ì…ë ¥ë“¤ì„ ì½ê³ \n",
    "    - buggy_code(put) ë° GenProgsVerifiedì˜ variantë“¤ì— ëŒ€í•´\n",
    "      ë˜í¼ë¥¼ í†µí•´ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•œë‹¤.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame(columns=['pid', 'input_name', 'sol_name', 'out', 'input_valid', 'out_correct'])\n",
    "    \"\"\"\n",
    "    df_rows: List[Dict[str, Any]] = []\n",
    "    inputs_base = Path(inputs_base_path)\n",
    "\n",
    "    for pid, problem in problems.items():\n",
    "        # ì–¸ì–´ëŠ” pythonë§Œ ì²˜ë¦¬\n",
    "        lang = problem.get(\"language\", \"python\")\n",
    "        if lang != \"python\":\n",
    "            print(f\"[Warning] PID {pid}: unsupported language {lang}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        pid_inputs_dir = inputs_base / pid\n",
    "        if not pid_inputs_dir.exists():\n",
    "            print(f\"[Warning] No test inputs for {pid} at {pid_inputs_dir}\")\n",
    "            continue\n",
    "\n",
    "        input_files = sorted(pid_inputs_dir.glob(\"*.in\"))\n",
    "        if not input_files:\n",
    "            print(f\"[Warning] No .in files for {pid} at {pid_inputs_dir}\")\n",
    "            continue\n",
    "\n",
    "        # ì†”ë£¨ì…˜ ëª©ë¡: buggy_code (put) + GenProgsVerified/{pid}/ ì•„ë˜ì˜ variant_*.py\n",
    "        sol_entries: List[tuple[Union[str, Path], str]] = []\n",
    "\n",
    "        buggy_code = problem.get(\"buggy_code\")\n",
    "        if buggy_code:\n",
    "            sol_entries.append((buggy_code, \"put\"))\n",
    "        else:\n",
    "            print(f\"[Warning] No buggy_code for pid={pid}\")\n",
    "\n",
    "        verified_dir = Path(inputs_base_path).parent / \"GenProgsVerified\" / pid\n",
    "        if verified_dir.exists():\n",
    "            for f in sorted(verified_dir.glob(\"*.py\")):\n",
    "                sol_entries.append((f, f.stem))\n",
    "\n",
    "        if not sol_entries:\n",
    "            print(f\"[Warning] No solutions (put or verified) for pid={pid}\")\n",
    "            continue\n",
    "\n",
    "        for input_file in input_files:\n",
    "            input_bytes = input_file.read_bytes()\n",
    "\n",
    "            for code, sol_name in sol_entries:\n",
    "                out = _run_quixbugs_on_input(\n",
    "                    pid=pid,\n",
    "                    problem=problem,\n",
    "                    code=code,\n",
    "                    sol_name=sol_name,\n",
    "                    input_bytes=input_bytes,\n",
    "                    timeout=timeout,\n",
    "                )\n",
    "\n",
    "                input_valid = out not in (b\"\", b\"TIMEOUT\")\n",
    "\n",
    "                df_rows.append(\n",
    "                    {\n",
    "                        \"pid\": pid,\n",
    "                        \"input_name\": input_file.name,\n",
    "                        \"sol_name\": sol_name,\n",
    "                        \"out\": out,\n",
    "                        \"input_valid\": input_valid,\n",
    "                        # ì—¬ê¸°ì„œëŠ” ì•„ì§ ì •ë‹µ ì—¬ë¶€ë¥¼ ëª¨ë¦„ â†’ TrickyBugs ë²„ì „ì²˜ëŸ¼\n",
    "                        # ìš°ì„  input_validë¡œ ì±„ì›Œë‘ê³ , ë‚˜ì¤‘ì— ì°¸ì¡° ì†”ë£¨ì…˜ê³¼ ë¹„êµí•´ out_correct ê³„ì‚° ê°€ëŠ¥\n",
    "                        \"out_correct\": input_valid,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(df_rows)\n",
    "    return df\n",
    "\n",
    "def find_variants_diff_from_put(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    task_oracle_quixbugs() ì˜ ì¶œë ¥ DataFrame(df)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,\n",
    "    ê°™ì€ (pid, input_name)ì— ëŒ€í•´ PUTê³¼ **ì„œë¡œ ë‹¤ë¥¸ out**ì„ ë‚´ëŠ”\n",
    "    variant ì†”ë£¨ì…˜ë“¤ë§Œ ê³¨ë¼ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        task_oracle_quixbugs() ê°€ ë°˜í™˜í•œ DataFrame.\n",
    "        ìµœì†Œí•œ ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "        - 'pid'\n",
    "        - 'input_name'\n",
    "        - 'sol_name'  (ê°’: 'put' ë˜ëŠ” 'variant_x' ë“±)\n",
    "        - 'out'       (bytes)\n",
    "        - 'input_valid'\n",
    "        - 'out_correct'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    diff_df : pd.DataFrame\n",
    "        PUTê³¼ ì¶œë ¥ì´ ë‹¤ë¥¸ variantì— ëŒ€í•œ í–‰ë§Œ ëª¨ì€ DataFrame.\n",
    "        ì»¬ëŸ¼:\n",
    "        - 'pid'\n",
    "        - 'input_name'\n",
    "        - 'sol_name'    (variant ì´ë¦„)\n",
    "        - 'out'         (variant ì¶œë ¥)\n",
    "        - 'put_out'     (PUT ì¶œë ¥)\n",
    "        - 'input_valid'\n",
    "        - 'out_correct'\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) PUT ì¶œë ¥ë§Œ ë”°ë¡œ ë½‘ì•„ì„œ (pid, input_name)ë³„ ê¸°ì¤€ê°’ìœ¼ë¡œ ì‚¬ìš©\n",
    "    put_df = (\n",
    "        df[df[\"sol_name\"] == \"put\"]\n",
    "        .loc[:, [\"pid\", \"input_name\", \"out\"]]\n",
    "        .rename(columns={\"out\": \"put_out\"})\n",
    "    )\n",
    "\n",
    "    # 2) variantë“¤ë§Œ ë¶„ë¦¬\n",
    "    var_df = df[df[\"sol_name\"] != \"put\"].copy()\n",
    "\n",
    "    if put_df.empty or var_df.empty:\n",
    "        # PUTì´ë‚˜ variantê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë¹„êµí•  ê²Œ ì—†ìŒ\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"pid\",\n",
    "                \"input_name\",\n",
    "                \"sol_name\",\n",
    "                \"out\",\n",
    "                \"put_out\",\n",
    "                \"input_valid\",\n",
    "                \"out_correct\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # 3) (pid, input_name) ê¸°ì¤€ìœ¼ë¡œ variant ìª½ì— PUT ê²°ê³¼ join\n",
    "    merged = var_df.merge(\n",
    "        put_df,\n",
    "        on=[\"pid\", \"input_name\"],\n",
    "        how=\"left\",      # ì¼ë¶€ ì…ë ¥ì— PUTì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ left\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    # 4) PUT ì¶œë ¥ì´ ì¡´ì¬í•˜ê³ (put_out not null),\n",
    "    #    variant out != put_out ì¸ ê²½ìš°ë§Œ í•„í„°ë§\n",
    "    mask_has_put = merged[\"put_out\"].notna()\n",
    "    mask_diff_out = merged[\"out\"] != merged[\"put_out\"]\n",
    "    diff_df = merged[mask_has_put & mask_diff_out].copy()\n",
    "\n",
    "    # 5) ê´€ì‹¬ ì»¬ëŸ¼ë§Œ ì •ë¦¬í•´ì„œ ë°˜í™˜\n",
    "    diff_df = diff_df[\n",
    "        [\n",
    "            \"pid\",\n",
    "            \"input_name\",\n",
    "            \"sol_name\",   # ì–´ë–¤ variantì¸ì§€\n",
    "            \"out\",        # variant ì¶œë ¥\n",
    "            \"put_out\",    # put ì¶œë ¥\n",
    "            \"input_valid\",\n",
    "            \"out_correct\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    return diff_df\n",
    "\n",
    "combination_cache = {}\n",
    "def Cnk(n, k):\n",
    "    if (n, k) in combination_cache:\n",
    "        return combination_cache[(n, k)]\n",
    "    result = math.comb(n, k)\n",
    "    combination_cache[(n, k)] = result\n",
    "    return result\n",
    "\n",
    "def get_trigger_df(df:pd.DataFrame,lang:str,method_type:str):\n",
    "    '''\n",
    "    method_type: 'dfp' or 'tc'\n",
    "    lang: 'cpp' or 'python'\n",
    "    '''\n",
    "    assert lang in ['cpp','python']\n",
    "    if method_type!='dfp' and method_type!='tc':\n",
    "        raise RuntimeError(f\"Wrong method_type: {method_type}\")\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['pid', 'input_name','out','sol_names','input_valid','input_valid_byref','out_correct'])\n",
    "\n",
    "    # A: the PUT\n",
    "    # B: other programs\n",
    "    if lang=='cpp':\n",
    "        A = df[df['sol_name'].str.startswith('sol_')]\n",
    "        B = df[~df['sol_name'].str.startswith('sol_')]\n",
    "    elif lang=='python':\n",
    "        A = df[~df['sol_name'].str.startswith('p0')]\n",
    "        B = df[df['sol_name'].str.startswith('p0')]\n",
    "    \n",
    "    A_outputs = A[['pid', 'input_name', 'out']].drop_duplicates()\n",
    "\n",
    "    # check B outputs with the same value\n",
    "    B_grouped = (B.groupby(['pid', 'input_name']))\n",
    "    totoal_len=len(B_grouped)\n",
    "    count=0\n",
    "    for (pid, input_name), group_df in B_grouped:\n",
    "        count+=1\n",
    "        if count%1000==0:\n",
    "            print(f\"get_triger: {count}/{totoal_len}\")\n",
    "        group_df=group_df.drop_duplicates(subset=['pid','input_name','sol_name','out'])\n",
    "        # deduplicate is import when multiple ref_out occur\n",
    "        # For Example:\n",
    "        # A[(A['pid']=='p02550') & (A['input_name']=='1_1_1.in.json')]\n",
    "        # pid\tinput_name\tsol_name\tout\tis_out_hash\tinput_valid\tnumber_of_sols\tref_out\tis_refout_hash\tinput_valid_byref\tout_correct\n",
    "        # 121\tp02550\t1_1_1.in.json\tsol_129.out\t0\tFalse\t0\t4\t104\tFalse\tFalse\tFalse\n",
    "        # 122\tp02550\t1_1_1.in.json\tsol_129.out\t0\tFalse\t0\t1\t0\tFalse\tFalse\tTrue\n",
    "        out_values = group_df['out'].values\n",
    "        unique_out_values = set(out_values)        \n",
    "\n",
    "        for out_value in unique_out_values:\n",
    "            \n",
    "            a_out_df=A_outputs[(A_outputs['pid'] == pid) & (A_outputs['input_name'] == input_name)]\n",
    "         \n",
    "            if len(a_out_df)<1 :\n",
    "                continue\n",
    "            a_out = A[(A['pid'] == pid) & (A['input_name'] == input_name)]['out'].values[0]\n",
    "            if out_value==a_out:\n",
    "                continue\n",
    "        # get sol_name with the same out\n",
    "            matching_sol_names = group_df[group_df['out']==out_value]['sol_name'].to_list()\n",
    "            try:\n",
    "                input_valid=group_df[group_df['out']==out_value]['input_valid'].to_list()[0]\n",
    "                input_valid_byref=group_df[group_df['out']==out_value]['input_valid_byref'].to_list()[0]\n",
    "                out_correct=group_df[group_df['out']==out_value]['out_correct'].to_list()[0]\n",
    "            except:\n",
    "                continue\n",
    "            if method_type=='dfp' and len(matching_sol_names) < 2 :\n",
    "                continue\n",
    "\n",
    "            matching_sol_names=tuple(matching_sol_names)\n",
    "            result_df.loc[len(result_df)] = [pid,input_name,out_value,matching_sol_names,input_valid,input_valid_byref,out_correct]\n",
    "            \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def compute_res_df(df_triger:pd.DataFrame,num_of_ref_progs:int,method_type:str):\n",
    "    assert(method_type in ['tc','dfp'])\n",
    "    res_df=pd.DataFrame(columns=['pid','total','TP','FP'])\n",
    "    ori_n=num_of_ref_progs\n",
    "    for pid in df_triger['pid'].unique():\n",
    "        # print(f\"{pid} start\")\n",
    "        num_of_ref_progs=ori_n\n",
    "        df_triger_pid=df_triger[df_triger['pid']==pid].copy()\n",
    "        sols_set = set()\n",
    "        df_triger_pid['sol_names'].apply(lambda x: sols_set.update( [sol_name.split('_')[1] for sol_name in x] ))\n",
    "        sols_list=sorted(list(sols_set))\n",
    "        total_sols_num=df_triger_pid['total_sols_num'].max()\n",
    "        if total_sols_num<num_of_ref_progs:\n",
    "            num_of_ref_progs=total_sols_num\n",
    "        total=Cnk(total_sols_num,num_of_ref_progs)\n",
    "\n",
    "        all_sols_name=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9']\n",
    "        out_sols_num=total_sols_num-len(sols_list)\n",
    "        if out_sols_num!=0:\n",
    "            out_sols_name=set(all_sols_name)-set(sols_list)\n",
    "            out_sols_name=list(out_sols_name)\n",
    "            out_sols_name.sort()\n",
    "            out_sols_list=out_sols_name[:out_sols_num]\n",
    "        else:\n",
    "            out_sols_list=[]\n",
    "        to_use_sols_name=sols_list+out_sols_list\n",
    "\n",
    "        combos=list(combinations(to_use_sols_name,num_of_ref_progs))\n",
    "        combos.sort()\n",
    "\n",
    "        if method_type=='dfp':\n",
    "            tp,fp=0,0\n",
    "            # pick num_of_ref_progs sols and check whether they are in the same group\n",
    "            # compute the average tp and fp among all inputs\n",
    "\n",
    "            for combo in combos:\n",
    "                combo=list(combo)\n",
    "        \n",
    "                df_triger_pid['combo_all_true']=df_triger_pid[combo].all(axis=1)\n",
    "                df_tmp=df_triger_pid[df_triger_pid['combo_all_true']]\n",
    "                if len(df_tmp)<1:\n",
    "                    continue\n",
    "                tp+= len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp+= 1-len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                if fp<0:\n",
    "                    print(f\"NOW TP:{tp},FP:{fp}\")\n",
    "                    print(len(df_tmp),len(df_tmp[df_tmp['final_valid']==True]),len(df_tmp[df_tmp['final_valid']==False]))\n",
    "                    \n",
    "            if tp==0 and fp==0:\n",
    "                continue\n",
    "            res_df.loc[len(res_df)]=[pid,total,tp,fp]\n",
    "        \n",
    "        elif method_type=='tc':\n",
    "            tp,fp=0,0\n",
    "            for combo in combos:\n",
    "                \n",
    "                combo=list(combo)    \n",
    "                df_triger_pid['combo_any_true']=df_triger_pid[combo].any(axis=1)\n",
    "                # first find all triger sol groups\n",
    "                df_tmp=df_triger_pid[df_triger_pid['combo_any_true']].copy()\n",
    "                if len(df_tmp)<1:\n",
    "                    continue\n",
    "                \n",
    "                df_tmp['sols_in_combo_num'] = df_tmp.loc[:,combo].sum(axis=1)\n",
    "                max_sols_in_combo = df_tmp.groupby('input_name')['sols_in_combo_num'].transform('max')\n",
    "                df_tmp=df_tmp[df_tmp['sols_in_combo_num']==max_sols_in_combo]\n",
    "                tp+=len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp+= 1 - len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "            if tp==0 and fp==0:\n",
    "                continue\n",
    "            res_df.loc[len(res_df)]=[pid,total,tp,fp]\n",
    "\n",
    "    \n",
    "    res_df['TP_rate']=res_df['TP']/res_df['total']\n",
    "    res_df['FP_rate']=res_df['FP']/res_df['total']\n",
    "    res_df['precision']=res_df['TP']/(res_df['TP']+res_df['FP'])\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def ep_get_ref_df(json_path:str):\n",
    "    df = pd.DataFrame(columns=['task_id', 'input_name', 'ref_out', 'input_valid'])\n",
    "    with open(json_path,\"r\") as f:\n",
    "        ref_json=json.load(f)\n",
    "    for task_key in ref_json:\n",
    "        this_task_json=ref_json[task_key]\n",
    "        task_id=task_key.replace('/','_')\n",
    "        inp_len=len(this_task_json)\n",
    "        new_rows=[]\n",
    "        for i in range(inp_len):\n",
    "            #print(f\"{task_id} input_{i}\")\n",
    "            valid=this_task_json[i][0]\n",
    "            output=this_task_json[i][1]\n",
    "            if valid==True:\n",
    "                new_row=[task_id,f\"input_{i}\",output,True]\n",
    "            else:\n",
    "                new_row=[task_id,f\"input_{i}\",None,False]\n",
    "            new_rows.append(new_row)\n",
    "        df_new_rows=pd.DataFrame(new_rows, columns=['task_id', 'input_name', 'ref_out', 'input_valid'])\n",
    "        df=pd.concat([df,df_new_rows],ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def to_hashable(obj):\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return tuple(to_hashable(item) for item in obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return frozenset((key, to_hashable(value)) for key, value in obj.items())\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def ep_get_trigger_df(df:pd.DataFrame,method_type:str):\n",
    "    assert method_type in ['dfp','tc']\n",
    "    df['out']=df['out'].apply(to_hashable)\n",
    "    result_df=pd.DataFrame(columns=['task_id', 'input_name','out','sol_names','input_valid','out_correct'])\n",
    "    A=df[(df['sol_name']=='put')]\n",
    "    B=df[~(df['sol_name']=='put')]\n",
    "    A_outputs=A[['task_id', 'input_name', 'out']].drop_duplicates()\n",
    "    B_grouped = (B.groupby(['task_id', 'input_name']))\n",
    "    totoal_len=len(B_grouped)\n",
    "    count=0\n",
    "    for (task_id, input_name), group_df in B_grouped:\n",
    "        count+=1\n",
    "        if count%1000==0:\n",
    "            print(f\"get_triger: {count}/{totoal_len}\")\n",
    "        group_df=group_df.drop_duplicates(subset=['task_id','input_name','sol_name','out'])\n",
    "        out_values = group_df['out'].values\n",
    "        unique_out_values = set(out_values) \n",
    "        for out_value in unique_out_values:\n",
    "            a_out_df=A_outputs[(A_outputs['task_id'] == task_id) & (A_outputs['input_name'] == input_name)]\n",
    "            if len(a_out_df)<1 :\n",
    "                continue\n",
    "            a_out = A[(A['task_id'] == task_id) & (A['input_name'] == input_name)]['out'].values[0]\n",
    "            if out_value==a_out:\n",
    "                continue\n",
    "            matching_sol_names = group_df[group_df['out']==out_value]['sol_name'].to_list()\n",
    "            input_valid=group_df[group_df['out']==out_value]['input_valid'].to_list()[0]\n",
    "            out_correct=group_df[group_df['out']==out_value]['out_correct'].to_list()[0]\n",
    "            if method_type=='dfp' and len(matching_sol_names) < 2 :\n",
    "                continue\n",
    "            matching_sol_names=tuple(matching_sol_names)\n",
    "            result_df.loc[len(result_df)] = [task_id,input_name,out_value,matching_sol_names,input_valid,out_correct]\n",
    "    result_df['len_sol_names']=result_df['sol_names'].apply(lambda x:len(x))\n",
    "    result_df['final_valid']= ( (result_df['input_valid'] == True) & (result_df['out_correct']) )\n",
    "    result_df['total_sols_num']=result_df['task_id'].apply(lambda x:len(df[df['task_id']==x].drop_duplicates(subset=['sol_name']))-1)\n",
    "    for i in range(10):\n",
    "        column_name=f\"num{i}\"\n",
    "        result_df[column_name] = result_df['sol_names'].apply(lambda x: 1 if f\"sol{i}\" in x else 0)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def ep_compute_res(df_triger:pd.DataFrame,num_of_ref_progs:int,method_type:str):\n",
    "    assert method_type in ['dfp','tc']\n",
    "    res_df=pd.DataFrame(columns=['task_id','total','TP','FP'])\n",
    "    ori_n=num_of_ref_progs\n",
    "    for task_id in df_triger['task_id'].unique():\n",
    "        num_of_ref_progs=ori_n\n",
    "        df_triger_pid=df_triger[df_triger['task_id']==task_id].copy()\n",
    "        sols_set = set()\n",
    "        df_triger_pid['sol_names'].apply(lambda x: sols_set.update( [f\"num{sol_name[-1]}\" for sol_name in x] ))\n",
    "        sols_list=sorted(list(sols_set))\n",
    "        total_sols_num=df_triger_pid['total_sols_num'].max()\n",
    "        if total_sols_num<num_of_ref_progs:\n",
    "            num_of_ref_progs=total_sols_num\n",
    "        total=Cnk(total_sols_num,num_of_ref_progs)\n",
    "        all_sols_name=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9']\n",
    "        out_sols_num=total_sols_num-len(sols_list)\n",
    "        if out_sols_num!=0:\n",
    "            out_sols_name=set(all_sols_name)-set(sols_list)\n",
    "            out_sols_name=list(out_sols_name)\n",
    "            out_sols_name.sort()\n",
    "            out_sols_list=out_sols_name[:out_sols_num]\n",
    "        else:\n",
    "            out_sols_list=[]    \n",
    "        to_use_sols_name=sols_list+out_sols_list\n",
    "\n",
    "        combos=list(combinations(to_use_sols_name,num_of_ref_progs))\n",
    "        combos.sort()\n",
    "        if method_type=='dfp':\n",
    "            tp,fp=0,0\n",
    "            for combo in combos:\n",
    "                combo=list(combo)\n",
    "                df_triger_pid['combo_all_true']=df_triger_pid[combo].all(axis=1)\n",
    "                df_tmp=df_triger_pid[df_triger_pid['combo_all_true']]\n",
    "                if len(df_tmp)<1:\n",
    "                    continue\n",
    "                tp+= len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp+= 1-len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                if fp<0:\n",
    "                    print(f\"NOW TP:{tp},FP:{fp}\")\n",
    "                    print(len(df_tmp),len(df_tmp[df_tmp['final_valid']==True]),len(df_tmp[df_tmp['final_valid']==False]))\n",
    "                    \n",
    "            if tp==0 and fp==0:\n",
    "                continue\n",
    "            res_df.loc[len(res_df)]=[task_id,total,tp,fp]\n",
    "        elif method_type=='tc':\n",
    "            tp,fp=0,0\n",
    "            for combo in combos:\n",
    "                combo=list(combo)    \n",
    "                df_triger_pid['combo_any_true']=df_triger_pid[combo].any(axis=1)\n",
    "                df_tmp=df_triger_pid[df_triger_pid['combo_any_true']].copy()\n",
    "                if len(df_tmp)<1:\n",
    "                    continue\n",
    "                \n",
    "                df_tmp['sols_in_combo_num'] = df_tmp.loc[:,combo].sum(axis=1)\n",
    "                max_sols_in_combo = df_tmp.groupby('input_name')['sols_in_combo_num'].transform('max')\n",
    "                df_tmp=df_tmp[df_tmp['sols_in_combo_num']==max_sols_in_combo]\n",
    "                tp+=len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp+= 1 - len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "            if tp==0 and fp==0:\n",
    "                continue\n",
    "            res_df.loc[len(res_df)]=[task_id,total,tp,fp]\n",
    "        else:\n",
    "            raise RuntimeError(f\"Wrong method_type: {method_type}\")\n",
    "    res_df['TP_rate']=res_df['TP']/res_df['total']\n",
    "    res_df['FP_rate']=res_df['FP']/res_df['total']\n",
    "    res_df['precision']=res_df['TP']/(res_df['TP']+res_df['FP'])\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def ep_can_compute_res(df_triger:pd.DataFrame,num_of_ref_progs:int,method_type:str):\n",
    "    assert method_type in ['dfp','tc']\n",
    "    res_df=pd.DataFrame(columns=['task_id','total','TP','FP','FP_bad_input'])\n",
    "    ori_n=num_of_ref_progs\n",
    "    for task_id in df_triger['task_id'].unique():\n",
    "        num_of_ref_progs=ori_n\n",
    "        df_triger_pid=df_triger[df_triger['task_id']==task_id].copy()\n",
    "        sols_set = set()\n",
    "        df_triger_pid['sol_names'].apply(lambda x: sols_set.update( [f\"num{sol_name[-1]}\" for sol_name in x] ))\n",
    "        sols_list=sorted(list(sols_set))\n",
    "        total_sols_num=df_triger_pid['total_sols_num'].max()\n",
    "        if total_sols_num<num_of_ref_progs:\n",
    "            num_of_ref_progs=total_sols_num\n",
    "        total=Cnk(total_sols_num,num_of_ref_progs)\n",
    "        all_sols_name=['num0','num1','num2','num3','num4','num5','num6','num7','num8','num9']\n",
    "        out_sols_num=total_sols_num-len(sols_list)\n",
    "        if out_sols_num!=0:\n",
    "            out_sols_name=set(all_sols_name)-set(sols_list)\n",
    "            out_sols_name=list(out_sols_name)\n",
    "            out_sols_name.sort()\n",
    "            out_sols_list=out_sols_name[:out_sols_num]\n",
    "        else:\n",
    "            out_sols_list=[]    \n",
    "        to_use_sols_name=sols_list+out_sols_list\n",
    "\n",
    "        combos=list(combinations(to_use_sols_name,num_of_ref_progs))\n",
    "        combos.sort()\n",
    "        if method_type=='dfp':\n",
    "            tp,fp=0,0\n",
    "            fp_bad_input=0\n",
    "            for combo in combos:\n",
    "                combo=list(combo)\n",
    "                df_triger_pid['combo_all_true']=df_triger_pid[combo].all(axis=1)\n",
    "                df_tmp=df_triger_pid[df_triger_pid['combo_all_true']]\n",
    "                if len(df_tmp)<1:\n",
    "                    continue\n",
    "                tp+= len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp+= 1-len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp_bad_input+=len(df_tmp[df_tmp['input_valid']!=True])/len(df_tmp)\n",
    "                if fp<0:\n",
    "                    print(f\"NOW TP:{tp},FP:{fp}\")\n",
    "                    print(len(df_tmp),len(df_tmp[df_tmp['final_valid']==True]),len(df_tmp[df_tmp['final_valid']==False]))\n",
    "                    \n",
    "            if tp==0 and fp==0:\n",
    "                continue\n",
    "            res_df.loc[len(res_df)]=[task_id,total,tp,fp,fp_bad_input]\n",
    "        elif method_type=='tc':\n",
    "            tp,fp=0,0\n",
    "            fp_bad_input=0\n",
    "            for combo in combos:\n",
    "                combo=list(combo)    \n",
    "                df_triger_pid['combo_any_true']=df_triger_pid[combo].any(axis=1)\n",
    "                df_tmp=df_triger_pid[df_triger_pid['combo_any_true']].copy()\n",
    "                if len(df_tmp)<1:\n",
    "                    continue\n",
    "                df_tmp['sols_in_combo_num'] = df_tmp.loc[:,combo].sum(axis=1)\n",
    "                max_sols_in_combo = df_tmp.groupby('input_name')['sols_in_combo_num'].transform('max')\n",
    "                df_tmp=df_tmp[df_tmp['sols_in_combo_num']==max_sols_in_combo]\n",
    "                tp+=len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp+= 1 - len(df_tmp[df_tmp['final_valid']==True])/len(df_tmp)\n",
    "                fp_bad_input+=len(df_tmp[df_tmp['input_valid']!=True])/len(df_tmp)\n",
    "            if tp==0 and fp==0:\n",
    "                continue\n",
    "            res_df.loc[len(res_df)]=[task_id,total,tp,fp,fp_bad_input]\n",
    "        else:\n",
    "            raise RuntimeError(f\"Wrong method_type: {method_type}\")\n",
    "    res_df['TP_rate']=res_df['TP']/res_df['total']\n",
    "    res_df['FP_rate']=res_df['FP']/res_df['total']\n",
    "    res_df['precision']=res_df['TP']/(res_df['TP']+res_df['FP'])\n",
    "    res_df['FP_bad_input_rate']=res_df['FP_bad_input']/res_df['total']\n",
    "    return res_df\n",
    "\n",
    "# # ----------------------------\n",
    "# # MAIN EXECUTION\n",
    "# # ----------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # ----------------------------\n",
    "#     # 1. TrickyBugs ë°ì´í„° ë¡œë“œ\n",
    "#     # ----------------------------\n",
    "#     base_path = \"/local_datasets/a2024105535/TrickCatcher/Datasets/TrickyBugs\"\n",
    "#     problems = load_trickybugs_data_revised(base_path, lang=\"cpp\")\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 2. ë³€í˜• ì½”ë“œ ìƒì„±\n",
    "#     # ----------------------------\n",
    "#     output_base_path = \"/local_datasets/a2024105535/TrickCatcher/Outputs\"\n",
    "#     num_variants = 3\n",
    "\n",
    "#     create_variants(problems, output_base_path, k=num_variants)\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 3. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±\n",
    "#     # ----------------------------\n",
    "#     num_test_inputs = 10\n",
    "#     generate_buggy_test_inputs(problems, output_base_path, num_inputs=num_test_inputs)\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 4. ë³€í˜• ì½”ë“œ ê²€ì¦\n",
    "#     # ----------------------------\n",
    "#     verify_variants(problems, output_base_path)\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # 5. í…ŒìŠ¤í¬ ì˜¤ë¼í´ í‰ê°€\n",
    "#     # ----------------------------\n",
    "#     ref_json_path = \"/local_datasets/a2024105535/TrickCatcher/Datasets/TrickyBugsRef/ref_outputs.json\"\n",
    "#     df_ref = ep_get_ref_df(ref_json_path)\n",
    "\n",
    "#     # GenProgsVerifiedì—ì„œ ê° ë¬¸ì œë³„ ë³€í˜• ì½”ë“œ ì¶œë ¥ ìˆ˜ì§‘\n",
    "#     variant_results = []\n",
    "#     inputs_base_path = Path(output_base_path) / \"chat_generated_inputs\"\n",
    "    \n",
    "#     df_variants = task_oracle(problems, str(inputs_base_path), lang=\"cpp\")\n",
    "\n",
    "#     # trigger df ìƒì„±\n",
    "#     df_trigger = ep_get_trigger_df(df_variants, method_type=\"dfp\")\n",
    "\n",
    "#     # ìµœì¢… í‰ê°€\n",
    "#     res_df = ep_compute_res(df_trigger, num_of_ref_progs=2, method_type=\"dfp\")\n",
    "\n",
    "#     print(\"\\nâœ… TrickCatcher í‰ê°€ ì™„ë£Œ!\")\n",
    "#     print(res_df.head())\n",
    "    \n",
    "#     # ê²°ê³¼ ì €ì¥\n",
    "#     result_save_path = Path(output_base_path) / \"evaluation_results.csv\"\n",
    "#     res_df.to_csv(result_save_path, index=False)\n",
    "#     print(f\"ğŸ“Š í‰ê°€ ê²°ê³¼ ì €ì¥: {result_save_path}\")\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN EXECUTION\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # ----------------------------\n",
    "    # 1. QuixBugs ë°ì´í„° ë¡œë“œ\n",
    "    # ----------------------------\n",
    "    extracted_code = extract_python_code_from_dirs(QUIXBUGS_ROOT_DIR, TARGET_DIRS)\n",
    "    quixbugs_problems = load_quixbugs_data_revised(extracted_code)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. ë³€í˜• ì½”ë“œ ìƒì„±\n",
    "    # ----------------------------\n",
    "    output_base_path = \"./Outputs\"\n",
    "    num_variants = 3\n",
    "    create_quixbugs_variants(quixbugs_problems, output_base_path, k=num_variants)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±\n",
    "    # ----------------------------\n",
    "    num_test_inputs = 10\n",
    "    generate_buggy_test_inputs_quixbugs(quixbugs_problems, output_base_path, num_inputs=num_test_inputs)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. ë³€í˜• ì½”ë“œ ê²€ì¦\n",
    "    # ---------------------------\n",
    "    output_base_path = \"./Outputs\"\n",
    "    verify_quixbugs_variants(quixbugs_problems, output_base_path, 5)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. í…ŒìŠ¤í¬ ì˜¤ë¼í´ ì œì‘\n",
    "    # ----------------------------\n",
    "    inputs_base_path = Path(output_base_path) / \"chat_generated_inputs\"\n",
    "    \n",
    "    df_triggers = task_oracle_quixbugs(\n",
    "        quixbugs_problems,\n",
    "        inputs_base_path=str(inputs_base_path),\n",
    "        timeout=5,\n",
    "    )\n",
    "    print(f\"ì´ {len(df_triggers)}ê°œì˜ (pid, input, sol) ê²°ê³¼ ìˆ˜ì§‘\")\n",
    "\n",
    "    print(\"\\nâœ… QuixBugs Oracle ìƒì„± ì™„ë£Œ!\")\n",
    "    result_save_path = Path(output_base_path) / \"oracle_results.csv\"\n",
    "    df_triggers.to_csv(result_save_path, index=False)\n",
    "    print(f\"ğŸ“Š Oracle ì €ì¥: {result_save_path}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. ì¦ê±° ì œì‘\n",
    "    # ----------------------------\n",
    "    evidences = find_variants_diff_from_put(df_triggers)\n",
    "    \n",
    "    # ì¦ê±° ì €ì¥\n",
    "    result_save_path = Path(output_base_path) / \"evidence_results.csv\"\n",
    "    evidences.to_csv(result_save_path, index=False)\n",
    "    print(f\"ğŸ“Š ì¦ê±° ì €ì¥: {result_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Outputs í•˜ìœ„ GenProgs, chat_generated_inputs í´ë” ë‚´ìš© í™•ì¸\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# GenProgs í´ë” í™•ì¸\n",
    "genprogs_path = Path(\"./Outputs\") / \"GenProgs\"\n",
    "if genprogs_path.exists():\n",
    "    print(f\"GenProgs í´ë” ë‚´ì˜ ë¬¸ì œ ìˆ˜: {len(list(genprogs_path.iterdir()))}\")\n",
    "    first_pid_dir = next(iter(genprogs_path.iterdir()), None)\n",
    "    if first_pid_dir and first_pid_dir.is_dir():\n",
    "        print(f\"ì²« ë²ˆì§¸ ë¬¸ì œ ({first_pid_dir.name}) ë‚´ì˜ íŒŒì¼ ìˆ˜: {len(list(first_pid_dir.glob('*.py')))}\")\n",
    "else:\n",
    "    print(\"GenProgs í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# chat_generated_inputs í´ë” í™•ì¸\n",
    "inputs_path = Path(\"./Outputs\") / \"chat_generated_inputs\"\n",
    "if inputs_path.exists():\n",
    "    print(f\"\\nchat_generated_inputs í´ë” ë‚´ì˜ ë¬¸ì œ ìˆ˜: {len(list(inputs_path.iterdir()))}\")\n",
    "    first_pid_dir = next(iter(inputs_path.iterdir()), None)\n",
    "    if first_pid_dir and first_pid_dir.is_dir():\n",
    "        print(f\"ì²« ë²ˆì§¸ ë¬¸ì œ ({first_pid_dir.name}) ë‚´ì˜ .in íŒŒì¼ ìˆ˜: {len(list(first_pid_dir.glob('*.in')))}\")\n",
    "else:\n",
    "    print(\"chat_generated_inputs í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dbc6c",
   "metadata": {},
   "source": [
    "# LLM + TrickCatcher Error Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e77c6d",
   "metadata": {},
   "source": [
    "## Error correction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f464d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_quixbugs_evidence_from_df(\n",
    "    diff_df: pd.DataFrame,\n",
    "    inputs_base_path: str,\n",
    ") -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    find_variants_diff_from_put() ê°€ ë°˜í™˜í•œ diff_dfì™€\n",
    "    LLMì´ ìƒì„±í•œ .in ì…ë ¥ íŒŒì¼ë“¤ì´ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì´ìš©í•´,\n",
    "\n",
    "        { pid: [ {input, put_output, variant_output, variant_name}, ... ] }\n",
    "\n",
    "    í˜•íƒœì˜ evidence_pairs ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•œë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    diff_df : pd.DataFrame\n",
    "        find_variants_diff_from_put() ì˜ ì¶œë ¥.\n",
    "        ìµœì†Œí•œ ë‹¤ìŒ ì»¬ëŸ¼ë“¤ì„ ê°€ì§„ë‹¤ê³  ê°€ì •:\n",
    "          - 'pid'\n",
    "          - 'input_name'\n",
    "          - 'sol_name'   (variant ì´ë¦„)\n",
    "          - 'out'        (variant ì¶œë ¥, bytes ë˜ëŠ” str)\n",
    "          - 'put_out'    (PUT ì¶œë ¥, bytes ë˜ëŠ” str)\n",
    "\n",
    "    inputs_base_path : str\n",
    "        .in íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë£¨íŠ¸ ë””ë ‰í† ë¦¬.\n",
    "        ì˜ˆ: \"./Outputs/chat_generated_inputs\"\n",
    "        ì‹¤ì œ ì…ë ¥ íŒŒì¼ ê²½ë¡œëŠ” {inputs_base_path}/{pid}/{input_name}\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    evidence_by_pid : Dict[str, List[Dict[str, str]]]\n",
    "        ê° pidë³„ evidence_pairs ë¦¬ìŠ¤íŠ¸.\n",
    "        ê° evidence dict í˜•ì‹:\n",
    "            {\n",
    "                \"input\": str,\n",
    "                \"put_output\": str,\n",
    "                \"variant_output\": str,\n",
    "                \"variant_name\": str,\n",
    "            }\n",
    "    \"\"\"\n",
    "    inputs_base = Path(inputs_base_path)\n",
    "    evidence_by_pid: Dict[str, List[Dict[str, str]]] = defaultdict(list)\n",
    "\n",
    "    for _, row in diff_df.iterrows():\n",
    "        pid = row[\"pid\"]\n",
    "        input_name = row[\"input_name\"]\n",
    "        sol_name = row[\"sol_name\"]\n",
    "\n",
    "        # 1) ì…ë ¥(.in) íŒŒì¼ ë‚´ìš© ì½ê¸°\n",
    "        input_path = inputs_base / pid / input_name\n",
    "        if not input_path.exists():\n",
    "            print(f\"[Warning] Input file not found: {input_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            input_str = input_path.read_text(encoding=\"utf-8\").strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Failed to read input file {input_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 2) PUT / variant ì¶œë ¥(bytes â†’ str)\n",
    "        put_out = row.get(\"put_out\", b\"\")\n",
    "        var_out = row.get(\"out\", b\"\")\n",
    "\n",
    "        if isinstance(put_out, bytes):\n",
    "            put_out_str = put_out.decode(\"utf-8\", errors=\"replace\").strip()\n",
    "        else:\n",
    "            put_out_str = str(put_out).strip()\n",
    "\n",
    "        if isinstance(var_out, bytes):\n",
    "            var_out_str = var_out.decode(\"utf-8\", errors=\"replace\").strip()\n",
    "        else:\n",
    "            var_out_str = str(var_out).strip()\n",
    "\n",
    "        evidence_by_pid[pid].append(\n",
    "            {\n",
    "                \"input\": input_str,\n",
    "                \"put_output\": put_out_str,\n",
    "                \"variant_output\": var_out_str,\n",
    "                \"variant_name\": str(sol_name),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return dict(evidence_by_pid)\n",
    "\n",
    "def repair_python_with_trickcatcher_evidence(\n",
    "    buggy_code: str,\n",
    "    evidence_pairs: List[Dict[str, str]],\n",
    "    max_new_tokens: int = 1280,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    TrickCatcherì—ì„œ ì–»ì€ ì¦ê±°(ì…ë ¥ + PUT ì¶œë ¥ + Variant ì¶œë ¥)ë¥¼ í™œìš©í•´\n",
    "    Qwen ëª¨ë¸ë¡œ ì—ëŸ¬ ìˆ˜ì •ëœ Python ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.\n",
    "    (ìì—°ì–´ ë¬¸ì œ ì„¤ëª… ì—†ì´, ì½”ë“œì™€ IO ì˜ˆì‹œë§Œ ì‚¬ìš©)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    buggy_code : str\n",
    "        ì—ëŸ¬ê°€ ìˆëŠ” ì›ë³¸ Python ì½”ë“œ (PUT ì½”ë“œ ì „ì²´ ë¬¸ìì—´).\n",
    "\n",
    "    evidence_pairs : List[Dict[str, str]]\n",
    "        TrickCatcherì—ì„œ ì¶”ë¦° \"PUTê³¼ Variantì˜ ì¶œë ¥ì´ ë‹¬ëë˜\" (input, output) ì¦ê±° ë¦¬ìŠ¤íŠ¸.\n",
    "        ê° ì›ì†ŒëŠ” ë‹¤ìŒ keyë“¤ì„ ê°–ëŠ” dictë¼ê³  ê°€ì •:\n",
    "            {\n",
    "                \"input\": str,            # í”„ë¡œê·¸ë¨ì— ì „ë‹¬í•œ ì…ë ¥ (stdin ë‚´ìš© ë˜ëŠ” ì¸ì í‘œí˜„)\n",
    "                \"put_output\": str,       # í•´ë‹¹ ì…ë ¥ì— ëŒ€í•œ PUT ì½”ë“œì˜ ì¶œë ¥\n",
    "                \"variant_output\": str,   # í•´ë‹¹ ì…ë ¥ì— ëŒ€í•œ Variant í”„ë¡œê·¸ë¨ì˜ ì¶œë ¥\n",
    "                # ì„ íƒì ìœ¼ë¡œ \"variant_name\": str ê°€ëŠ¥\n",
    "            }\n",
    "\n",
    "    max_new_tokens : int, optional\n",
    "        LLMì´ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ê¸°ë³¸ê°’ 1280.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Qwenì´ ìƒì„±í•œ \"ìˆ˜ì •ëœ Python ì½”ë“œ\" ë¬¸ìì—´.\n",
    "        (generate_code_responseë¥¼ ê±°ì³ ì½”ë“œ ë¶€ë¶„ë§Œ ì¶”ì¶œëœ ìƒíƒœ)\n",
    "    \"\"\"\n",
    "\n",
    "    # ì¦ê±° ì˜ˆì‹œëŠ” ë„ˆë¬´ ë§ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ì–´ì§€ë¯€ë¡œ ìƒí•œì„ ë‘ \n",
    "    MAX_EXAMPLES = 10\n",
    "    used_pairs = evidence_pairs[:MAX_EXAMPLES]\n",
    "\n",
    "    examples_text_lines = []\n",
    "    for idx, ex in enumerate(used_pairs, start=1):\n",
    "        input_str = ex.get(\"input\", \"\").strip()\n",
    "        put_out = ex.get(\"put_output\", \"\").strip()\n",
    "        var_out = ex.get(\"variant_output\", \"\").strip()\n",
    "        var_name = ex.get(\"variant_name\", \"variant\")\n",
    "\n",
    "        examples_text_lines.append(\n",
    "            f\"Example {idx}:\\n\"\n",
    "            f\"  INPUT:\\n{input_str}\\n\\n\"\n",
    "            f\"  PUT OUTPUT (buggy):\\n{put_out}\\n\\n\"\n",
    "            f\"  {var_name} OUTPUT:\\n{var_out}\\n\"\n",
    "            \"----------------------------------------\"\n",
    "        )\n",
    "\n",
    "    examples_block = \"\\n\\n\".join(examples_text_lines) if examples_text_lines else \"No concrete examples provided.\"\n",
    "\n",
    "    # LLMì—ê²Œ ì¤„ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Python ì½”ë“œë§Œ ë°˜í™˜í•˜ë„ë¡ ê°•í•˜ê²Œ ì§€ì‹œ)\n",
    "    # ì£¼ì˜: extract_codeê°€ ``` ë˜ëŠ” ```cpp ë§Œ ì¡ê¸° ë•Œë¬¸ì—, ì—¬ê¸°ì„œëŠ” ì–¸ì–´ íƒœê·¸ ì—†ì´ ```ë§Œ ì‚¬ìš©\n",
    "    prompt = f\"\"\"You are a professional Python debugging and repair assistant.\n",
    "\n",
    "You will receive:\n",
    "1) A buggy Python program (called PUT).\n",
    "2) Several test inputs where the PUT's output disagrees with another variant program's output.\n",
    "\n",
    "Your task:\n",
    "- Infer the correct intended behavior using ONLY the buggy code and the IO examples.\n",
    "- Repair the PUT code so that it becomes logically correct and matches the intended behavior.\n",
    "- Preserve the function signatures and the input/output format (including stdin / stdout behavior).\n",
    "- Avoid undefined behavior and keep the code reasonably clean and readable.\n",
    "- Make sure the fixed Python code is executable as-is.\n",
    "\n",
    "VERY IMPORTANT OUTPUT FORMAT:\n",
    "- Your response MUST contain only the complete fixed Python code,\n",
    "- WRAPPED INSIDE ONE SINGLE MARKDOWN CODE BLOCK.\n",
    "- Start your response IMMEDIATELY with ``` on the first line,\n",
    "  and end your response IMMEDIATELY with ``` on the last line.\n",
    "- Do NOT add any explanation, comments, or text outside the code block.\n",
    "\n",
    "==================== BUGGY PYTHON CODE (PUT) ====================\n",
    "{buggy_code}\n",
    "\n",
    "==================== DISAGREEMENT EXAMPLES ====================\n",
    "Below are inputs where the PUT output differs from another variant's output.\n",
    "\n",
    "{examples_block}\n",
    "\n",
    "Now, output ONLY the fully repaired Python program in a single ``` code block.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆëŠ” generate_code_responseë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    fixed_code = generate_code_response(messages, max_new_tokens=max_new_tokens)\n",
    "    return fixed_code\n",
    "\n",
    "def batch_repair_python_with_trickcatcher_evidence(\n",
    "    problem_names: List[str],\n",
    "    buggy_codes: List[str],\n",
    "    all_evidence_pairs: List[List[Dict[str, str]]],\n",
    "    max_new_tokens: int = 1280,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    ì—¬ëŸ¬ ë¬¸ì œì— ëŒ€í•´\n",
    "      - ë¬¸ì œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "      - ê° ë¬¸ì œì˜ buggy Python ì½”ë“œ\n",
    "      - ê° ë¬¸ì œì— ëŒ€í•œ evidence_pairs ë¦¬ìŠ¤íŠ¸\n",
    "    ë¥¼ ë°›ì•„, Qwen ê¸°ë°˜ error correctionì„ ìˆ˜í–‰í•˜ê³ \n",
    "\n",
    "        {ë¬¸ì œì´ë¦„: ìˆ˜ì •ëœ ì½”ë“œ ë¬¸ìì—´}\n",
    "\n",
    "    í˜•íƒœì˜ dictë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem_names : List[str]\n",
    "        ê° ë¬¸ì œì˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [\"HumanEval/0\", \"HumanEval/1\", ...])\n",
    "\n",
    "    buggy_codes : List[str]\n",
    "        ê° ë¬¸ì œì— ëŒ€í•œ buggy Python ì½”ë“œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸.\n",
    "        problem_names[i]ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œëŠ” buggy_codes[i].\n",
    "\n",
    "    all_evidence_pairs : List[List[Dict[str, str]]]\n",
    "        ê° ë¬¸ì œì— ëŒ€í•œ evidence_pairs ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸.\n",
    "        all_evidence_pairs[i]ëŠ” problem_names[i], buggy_codes[i]ì— ëŒ€ì‘í•˜ëŠ”\n",
    "        evidence_pairs (List[Dict])ì…ë‹ˆë‹¤.\n",
    "\n",
    "    max_new_tokens : int, optional\n",
    "        ê° ë¬¸ì œì— ëŒ€í•´ LLMì´ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        key: ë¬¸ì œ ì´ë¦„ (problem_names[i])\n",
    "        value: í•´ë‹¹ ë¬¸ì œì— ëŒ€í•´ LLMì´ ìƒì„±í•œ ìˆ˜ì •ëœ Python ì½”ë“œ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "\n",
    "    if not (len(problem_names) == len(buggy_codes) == len(all_evidence_pairs)):\n",
    "        raise ValueError(\n",
    "            \"problem_names, buggy_codes, all_evidence_pairs ê¸¸ì´ê°€ ì„œë¡œ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "        )\n",
    "\n",
    "    fixed_code_dict: Dict[str, str] = {}\n",
    "\n",
    "    for name, buggy_code, evidence_pairs in zip(problem_names, buggy_codes, all_evidence_pairs):\n",
    "        print(f\"\\n=== Repairing problem: {name} ===\")\n",
    "        fixed_code = repair_python_with_trickcatcher_evidence(\n",
    "            buggy_code=buggy_code,\n",
    "            evidence_pairs=evidence_pairs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "        fixed_code_dict[name] = fixed_code\n",
    "\n",
    "    return fixed_code_dict\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) quixbugs_problems + diff_df ë¡œ batch error correction ìˆ˜í–‰\n",
    "#    (ì´ë¯¸ ìˆëŠ” repair_python_with_trickcatcher_evidence ì´ìš©)\n",
    "# -------------------------------------------------------------------\n",
    "def batch_repair_quixbugs_with_evidence_from_df(\n",
    "    quixbugs_problems: Dict[str, Dict[str, Any]],\n",
    "    diff_df: pd.DataFrame,\n",
    "    inputs_base_path: str,\n",
    "    max_new_tokens: int = 1280,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    QuixBugs ì „ìš© batch error correction í—¬í¼.\n",
    "\n",
    "    1) diff_df(find_variants_diff_from_put ê²°ê³¼) + .in íŒŒì¼ë“¤ë¡œë¶€í„°\n",
    "       pidë³„ evidence_pairs ë¥¼ ë§Œë“  ë’¤,\n",
    "    2) ê° pidì— ëŒ€í•´ buggy_code + evidence_pairs ë¥¼ ì´ìš©í•˜ì—¬\n",
    "       Qwen ê¸°ë°˜ error correction ìˆ˜í–‰.\n",
    "\n",
    "    ë‚´ë¶€ì ìœ¼ë¡œ:\n",
    "      - build_quixbugs_evidence_from_df()\n",
    "      - repair_python_with_trickcatcher_evidence()\n",
    "      ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    quixbugs_problems : Dict[str, Dict[str, Any]]\n",
    "        load_quixbugs_data_revised() ì˜ ì¶œë ¥.\n",
    "        ì˜ˆ:\n",
    "        {\n",
    "          \"bitcount\": {\n",
    "              \"language\": \"python\",\n",
    "              \"buggy_code\": \"...\",\n",
    "              \"test_code\": \"...\",\n",
    "              \"meta\": {...},\n",
    "          },\n",
    "          ...\n",
    "        }\n",
    "\n",
    "    diff_df : pd.DataFrame\n",
    "        find_variants_diff_from_put() ì˜ ê²°ê³¼.\n",
    "        (PUTê³¼ ì¶œë ¥ì´ ë‹¬ëë˜ variant í–‰ë“¤ë§Œ í¬í•¨ëœ DF)\n",
    "\n",
    "    inputs_base_path : str\n",
    "        LLM ìƒì„± .in íŒŒì¼ ë£¨íŠ¸ ë””ë ‰í† ë¦¬.\n",
    "        ì˜ˆ: \"./Outputs/chat_generated_inputs\"\n",
    "\n",
    "    max_new_tokens : int\n",
    "        ê° ë¬¸ì œì— ëŒ€í•´ Qwenì´ ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        key: pid (ë¬¸ì œ ì´ë¦„, ì˜ˆ: \"bitcount\")\n",
    "        value: í•´ë‹¹ ë¬¸ì œì— ëŒ€í•´ LLMì´ ìƒì„±í•œ ìˆ˜ì •ëœ Python ì½”ë“œ ë¬¸ìì—´\n",
    "    \"\"\"\n",
    "    # 1) pidë³„ evidence_pairs êµ¬ì„±\n",
    "    evidence_by_pid = build_quixbugs_evidence_from_df(\n",
    "        diff_df=diff_df,\n",
    "        inputs_base_path=inputs_base_path,\n",
    "    )\n",
    "\n",
    "    # 2) batch_repair_python_with_trickcatcher_evidenceì— ë„£ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤ êµ¬ì„±\n",
    "    problem_names: List[str] = []\n",
    "    buggy_codes: List[str] = []\n",
    "    all_evidence_pairs: List[List[Dict[str, str]]] = []\n",
    "\n",
    "    for pid, problem in quixbugs_problems.items():\n",
    "        buggy_code = problem.get(\"buggy_code\")\n",
    "        if not buggy_code:\n",
    "            print(f\"[Warning] No buggy_code for pid={pid}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        evidence_pairs = evidence_by_pid.get(pid, [])\n",
    "        if not evidence_pairs:\n",
    "            # ì´ pidì— ëŒ€í•´ PUTê³¼ ë‹¤ë¥¸ ì¶œë ¥ì„ ë‚¸ variant ì¦ê±°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µí•˜ê±°ë‚˜,\n",
    "            # í•„ìš”ì‹œ ë¹ˆ evidenceë¡œë„ ì‹œë„í•  ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ)\n",
    "            print(f\"[Info] No evidence pairs for pid={pid}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        problem_names.append(pid)\n",
    "        buggy_codes.append(buggy_code)\n",
    "        all_evidence_pairs.append(evidence_pairs)\n",
    "\n",
    "    # 3) ê¸°ì¡´ batch_repair_python_with_trickcatcher_evidence ì¬ì‚¬ìš©\n",
    "    fixed_code_dict = batch_repair_python_with_trickcatcher_evidence(\n",
    "        problem_names=problem_names,\n",
    "        buggy_codes=buggy_codes,\n",
    "        all_evidence_pairs=all_evidence_pairs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    return fixed_code_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9896d2",
   "metadata": {},
   "source": [
    "# Make error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] No evidence pairs for pid=bitcount, skipping.\n",
      "[Info] No evidence pairs for pid=breadth_first_search, skipping.\n",
      "[Info] No evidence pairs for pid=bucketsort, skipping.\n",
      "[Info] No evidence pairs for pid=depth_first_search, skipping.\n",
      "[Info] No evidence pairs for pid=detect_cycle, skipping.\n",
      "[Info] No evidence pairs for pid=find_first_in_sorted, skipping.\n",
      "[Info] No evidence pairs for pid=find_in_sorted, skipping.\n",
      "[Info] No evidence pairs for pid=flatten, skipping.\n",
      "[Info] No evidence pairs for pid=gcd, skipping.\n",
      "[Info] No evidence pairs for pid=get_factors, skipping.\n",
      "[Info] No evidence pairs for pid=hanoi, skipping.\n",
      "[Info] No evidence pairs for pid=is_valid_parenthesization, skipping.\n",
      "[Info] No evidence pairs for pid=kheapsort, skipping.\n",
      "[Info] No evidence pairs for pid=knapsack, skipping.\n",
      "[Info] No evidence pairs for pid=kth, skipping.\n",
      "[Info] No evidence pairs for pid=lcs_length, skipping.\n",
      "[Info] No evidence pairs for pid=levenshtein, skipping.\n",
      "[Info] No evidence pairs for pid=lis, skipping.\n",
      "[Info] No evidence pairs for pid=longest_common_subsequence, skipping.\n",
      "[Info] No evidence pairs for pid=max_sublist_sum, skipping.\n",
      "[Info] No evidence pairs for pid=mergesort, skipping.\n",
      "[Info] No evidence pairs for pid=minimum_spanning_tree, skipping.\n",
      "[Info] No evidence pairs for pid=next_palindrome, skipping.\n",
      "[Info] No evidence pairs for pid=next_permutation, skipping.\n",
      "[Info] No evidence pairs for pid=node, skipping.\n",
      "[Info] No evidence pairs for pid=pascal, skipping.\n",
      "[Info] No evidence pairs for pid=possible_change, skipping.\n",
      "[Info] No evidence pairs for pid=powerset, skipping.\n",
      "[Info] No evidence pairs for pid=quicksort, skipping.\n",
      "[Info] No evidence pairs for pid=reverse_linked_list, skipping.\n",
      "[Info] No evidence pairs for pid=rpn_eval, skipping.\n",
      "[Info] No evidence pairs for pid=shortest_path_length, skipping.\n",
      "[Info] No evidence pairs for pid=shortest_path_lengths, skipping.\n",
      "[Info] No evidence pairs for pid=shortest_paths, skipping.\n",
      "[Info] No evidence pairs for pid=shunting_yard, skipping.\n",
      "[Info] No evidence pairs for pid=sieve, skipping.\n",
      "[Info] No evidence pairs for pid=sqrt, skipping.\n",
      "[Info] No evidence pairs for pid=subsequences, skipping.\n",
      "[Info] No evidence pairs for pid=to_base, skipping.\n",
      "[Info] No evidence pairs for pid=topological_ordering, skipping.\n",
      "[Info] No evidence pairs for pid=wrap, skipping.\n",
      "[INFO] error correctionì´ ìˆ˜í–‰ëœ ë¬¸ì œ ìˆ˜: 0\n",
      "\n",
      "ğŸ¯ ëª¨ë“  QuixBugs ë¬¸ì œì— ëŒ€í•œ evidence ê¸°ë°˜ error correction ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "#output_base_path = \"./Outputs\"\n",
    "\n",
    "# LLMì´ ìƒì„±í•œ .in ì…ë ¥ë“¤ì´ ë“¤ì–´ìˆëŠ” í´ë”\n",
    "inputs_base_path = Path(output_base_path) / \"chat_generated_inputs\"\n",
    "\n",
    "# error correction ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë”\n",
    "repaired_base = Path(output_base_path) / \"RepairedFromEvidence\"\n",
    "repaired_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "evidences = pd.read_csv(\"./Outputs/evidence_results.csv\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. evidences(= diff_df) ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ë¬¸ì œ error correction ìˆ˜í–‰\n",
    "# ----------------------------------------\n",
    "fixed_codes = batch_repair_quixbugs_with_evidence_from_df(\n",
    "    quixbugs_problems=quixbugs_problems,  # load_quixbugs_data_revised() ê²°ê³¼\n",
    "    diff_df=evidences,                    # find_variants_diff_from_put(df_triggers)ì˜ ê²°ê³¼\n",
    "    inputs_base_path=str(inputs_base_path),\n",
    "    max_new_tokens=1280,\n",
    ")\n",
    "\n",
    "print(f\"[INFO] error correctionì´ ìˆ˜í–‰ëœ ë¬¸ì œ ìˆ˜: {len(fixed_codes)}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. ê° ë¬¸ì œë³„ë¡œ ìˆ˜ì •ëœ ì½”ë“œ ì €ì¥\n",
    "#    ì˜ˆ: ./Outputs/RepairedFromEvidence/bitcount_repaired.py\n",
    "# ----------------------------------------\n",
    "for pid, code_str in fixed_codes.items():\n",
    "    save_path = repaired_base / f\"{pid}_repaired.py\"\n",
    "    save_path.write_text(code_str, encoding=\"utf-8\")\n",
    "    print(f\"  âœ… Saved repaired code for {pid} -> {save_path}\")\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë“  QuixBugs ë¬¸ì œì— ëŒ€í•œ evidence ê¸°ë°˜ error correction ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b5915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc4c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed7c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c795f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d16e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
